{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Methods in Text Analytics\n",
    "# Exercise 8: LLMs - Part 2\n",
    "### Daniel Ruffinelli\n",
    "## FSS 2025\n",
    "\n",
    "* This notebook is designed so we can test basic functions of LLMs in CPU using a regular laptop. For that reason, we stick to small models. But if you have better resources, feel free to modify this to any model that is [available in HuggingFace](https://huggingface.co/models).  \n",
    "* You run this code, you will need to install HuggingFace's [transformers](https://huggingface.co/docs/transformers/en/installation) and [PyTorch](https://pytorch.org/).\n",
    "* You will also need to do the following three things:\n",
    "1. Create a user name in HuggingFace.\n",
    "2. Request access to the following models: [Llama-3.1-1B](https://huggingface.co/meta-llama/Llama-3.2-1B) and [Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct).\n",
    "3. Create an access token, see [here](https://huggingface.co/docs/hub/security-tokens) for instructions. Your access token will be shown to you only once, so make you you copy it somewhere safe, because you will need to use it to login to HuggingFace via this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF login\n",
    "from huggingface_hub import notebook_login\n",
    "access_token = \"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\"\n",
    "\n",
    "# then run this and enter your token (requires ipywidgets) \n",
    "# alternatively, do it via CLI with huggingface-cli login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convenience, we'll store models and their corresponding tokenizers in a \n",
    "# dict of the form {model_name: [model, tokenizer]}\n",
    "from collections import defaultdict as ddict\n",
    "\n",
    "models_dict = ddict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set up some constants for convenience\n",
    "DEVICE=\"cpu\"\n",
    "MODEL = 0\n",
    "TOKENIZER = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# see all available models in HF here: https://huggingface.co/models\n",
    "# first time you load a model, it will be downloaded, which will take several\n",
    "# minutes, but after that, it will be read from a local cache, so it will be \n",
    "# only a few seconds\n",
    "\n",
    "# we load Llama-3.2-1B \n",
    "llama_name = \"meta-llama/Llama-3.2-1B\"\n",
    "models_dict[\"llama\"].append(\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        llama_name, \n",
    "        device_map=DEVICE, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "    )\n",
    ")\n",
    "models_dict[\"llama\"].append(\n",
    "    AutoTokenizer.from_pretrained(\n",
    "        llama_name, padding_side=\"left\"\n",
    "        )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load a model similar to GPT-3 made by EleutherAI\n",
    "gpt3_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "models_dict[\"gpt3\"].append(\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        gpt3_name, \n",
    "        device_map=DEVICE, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "    )\n",
    ")\n",
    "models_dict[\"gpt3\"].append(\n",
    "    AutoTokenizer.from_pretrained(\n",
    "        gpt3_name, padding_side=\"left\"\n",
    "        )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see loaded models\n",
    "for model_name, model in models_dict.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Model: {model[MODEL]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model and tokenizer to use\n",
    "model_name = \"gpt3\"\n",
    "model = models_dict[model_name][MODEL]\n",
    "tokenizer = models_dict[model_name][TOKENIZER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set toy prompt\n",
    "prompt = \"\"\"Hello!\"\"\"\n",
    "\n",
    "# tokenize it \n",
    "tokenized_prompt = tokenizer(\n",
    "    prompt, \n",
    "    return_tensors=\"pt\"\n",
    ").to(DEVICE)\n",
    "print(tokenized_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model predictions\n",
    "model_predictions = model(**tokenized_prompt)\n",
    "print(model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_top_k_tokens(prompt, model_tokenizer, k=10):\n",
    "    \"\"\"\n",
    "    Returns top k tokens predicted by the given tuple of model-tokenizer and \n",
    "    given prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    # unpacking\n",
    "    model = model_tokenizer[MODEL]\n",
    "    tokenizer = model_tokenizer[TOKENIZER]\n",
    "\n",
    "    # tokenizer prompt\n",
    "    tokenized_prompt = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # forward pass\n",
    "    model_predictions = model(**tokenized_prompt)\n",
    "\n",
    "    # get top k tokens\n",
    "    top_10_tokens = None\n",
    "    \n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    return top_10_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your function\n",
    "prompt = \"Hello?\"\n",
    "top_10_tokens = get_top_k_tokens(prompt, models_dict[\"gpt3\"], k=10)\n",
    "print(top_10_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "model_name = \"gpt3\"\n",
    "\n",
    "# ask basic questions\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# get top tokens\n",
    "top_10_tokens = get_top_k_tokens(prompt, models_dict[model_name])\n",
    "\n",
    "print(\"MODEL:\", model_name)\n",
    "print(f\"PROMPT:\\n{prompt}\")\n",
    "print(\"TOP 10 TOKENS:\", top_10_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_demonstrations_world_capitals():\n",
    "    \"\"\"\n",
    "    Task: World capitals.\n",
    "    \"\"\"\n",
    "\n",
    "    questions = [\n",
    "        \"Portugal\",\n",
    "        \"Germany\",\n",
    "        \"Italy\",\n",
    "        \"Spain\",\n",
    "        \"Poland\",\n",
    "        \"France\",\n",
    "    ]\n",
    "\n",
    "    answers = [\n",
    "        \"Lisbon\",\n",
    "        \"Berlin\",\n",
    "        \"Rome\",\n",
    "        \"Madrid\",\n",
    "        \"Warsaw\",\n",
    "        \"Paris\",\n",
    "    ]\n",
    "\n",
    "    return questions, answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demonstrations_verb_declination():\n",
    "    \"\"\"\n",
    "    Task: Verb declination in English.\n",
    "    \"\"\"\n",
    "\n",
    "    questions = [\n",
    "        \"I go, he \",\n",
    "        \"I play, he \",\n",
    "        \"I eat, he \",\n",
    "        \"You swim, she \",\n",
    "        \"You sleep, she \",\n",
    "        \"You sing, she \",\n",
    "        \"We say, she \",\n",
    "        \"We study, she \",\n",
    "        \"We pay, she \",\n",
    "    ]\n",
    "\n",
    "    answers = [\n",
    "        \"goes\",\n",
    "        \"plays\",\n",
    "        \"eats\",\n",
    "        \"swims\",\n",
    "        \"sleeps\",\n",
    "        \"sings\",\n",
    "        \"says\",\n",
    "        \"studies\",\n",
    "        \"pays\",\n",
    "    ]\n",
    "\n",
    "    return questions, answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demonstrations_ioi():\n",
    "    \"\"\"\n",
    "    Task: indirect object identification.\n",
    "    \"\"\"\n",
    "\n",
    "    questions = [\n",
    "        \"When Mary and John went to the store, John gave a drink to \", \n",
    "        \"Alice and Bob were playing chess. Alice won the game against \",\n",
    "        \"Harry and Hermione were studing in the library. Harry passed the book to \",\n",
    "    ]\n",
    "\n",
    "    answers = [\n",
    "        \"Mary\",\n",
    "        \"Bob\",\n",
    "        \"Hermione\",\n",
    "    ]\n",
    "    \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demonstrations_translate_to_french():\n",
    "    \"\"\"\n",
    "    Task: translate to French.\n",
    "    \"\"\"\n",
    "\n",
    "    questions = [\n",
    "        \"Car\",\n",
    "        \"House\",\n",
    "        \"Dog\",\n",
    "        \"Cat\", \n",
    "    ]\n",
    "\n",
    "    answers = [\n",
    "        \"Voiture\",\n",
    "        \"Maison\",\n",
    "        \"Chien\",\n",
    "        \"Chat\",\n",
    "    ]\n",
    "    \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demonstrations_translate_to_german():\n",
    "    \"\"\"\n",
    "    Task: translate to German.\n",
    "    \"\"\"\n",
    "\n",
    "    questions = [\n",
    "        \"Car\",\n",
    "        \"House\",\n",
    "        \"Dog\",\n",
    "        \"Cat\",\n",
    "    ]\n",
    "\n",
    "    answers = [\n",
    "        \"Auto\",\n",
    "        \"Haus\",\n",
    "        \"Hund\",\n",
    "        \"Katze\",\n",
    "    ]\n",
    "    \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demonstrations_translate_to_spanish():\n",
    "    \"\"\"\n",
    "    Task: translate to Spanish.\n",
    "    \"\"\"\n",
    "\n",
    "    questions = [\n",
    "        \"Car\"\n",
    "        \"House\"\n",
    "        \"Dog\"\n",
    "        \"Cat\",\n",
    "    ]\n",
    "\n",
    "    answers = [\n",
    "        \"Automovil\",\n",
    "        \"Casa\",\n",
    "        \"Perro\",\n",
    "        \"Gato\",\n",
    "    ]\n",
    "    \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_icl_prompt(\n",
    "        questions, \n",
    "        answers, \n",
    "        qa_template,\n",
    "        instruction=None,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Constructs an in-context learning (ICL) prompt.\n",
    "\n",
    "    Args:\n",
    "        questions (list): List of questions, all but the last will be used\n",
    "            as demonstrations in the prompt, whereas the last one will be\n",
    "            the question to be answered.\n",
    "        answers (list): corresponding answers to given set of questions.\n",
    "        instruction (str): Instruction to be used in the prompt.\n",
    "        qa_template (bool): If True, demonstrations are of the form \n",
    "                            Q: <question>. A: <answer>. If False, demonstrations \n",
    "                            are of the form <question>:<answer>. \n",
    "    Returns:\n",
    "        str: ICL prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\n",
    "    if instruction is not None:\n",
    "        prompt = instruction + \"\\n\\n\"\n",
    "    if qa_template:\n",
    "        for i, question in enumerate(questions[:-1]):\n",
    "            prompt += f\"Q: {question}\\nA: {answers[i]}\\n\\n\"\n",
    "        prompt += f\"Q: {questions[-1]}\\nA:\"\n",
    "    else:\n",
    "        for i, question in enumerate(questions[:-1]):\n",
    "            prompt += f\"{question}:{answers[i]}\\n\"\n",
    "        prompt += f\"{questions[-1]}:\"\n",
    "\n",
    "    return prompt, answers[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this function to sample and shuffle demonstrations\n",
    "def sample_and_shuffle_demonstrations(questions, answers, num_demos):\n",
    "    if num_demos > len(questions):\n",
    "        raise ValueError(\n",
    "            f\"Number of demonstrations ({num_demos}) is greater than the number of questions ({len(questions)})\"\n",
    "        )\n",
    "    sampled_questions = []\n",
    "    sampled_answers = []\n",
    "    for i in range(num_demos):\n",
    "        sampled_index = random.randint(0, len(questions) - 1)\n",
    "        sampled_questions.append(questions[sampled_index])\n",
    "        sampled_answers.append(answers[sampled_index])\n",
    "        questions.pop(sampled_index)\n",
    "        answers.pop(sampled_index)\n",
    "\n",
    "    return sampled_questions, sampled_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test ICL prompts\n",
    "num_demos = 1\n",
    "questions, answers = get_demonstrations_world_capitals()\n",
    "questions, answers = sample_and_shuffle_demonstrations(\n",
    "    questions, answers, num_demos\n",
    ")\n",
    "icl_prompt, answer = construct_icl_prompt(\n",
    "    questions, \n",
    "    answers,\n",
    "    qa_template=True \n",
    ")\n",
    "print(icl_prompt)\n",
    "print(\"\\nANSWER:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "model_name = \"gpt3\"\n",
    "num_demos = 1\n",
    "qa_template=False\n",
    "questions, answers = get_demonstrations_world_capitals()\n",
    "\n",
    "# construct ICL prompt\n",
    "questions, answers = sample_and_shuffle_demonstrations(\n",
    "    questions, answers, num_demos\n",
    ")\n",
    "icl_prompt, answer = construct_icl_prompt(\n",
    "    questions, \n",
    "    answers, \n",
    "    qa_template=qa_template,\n",
    ")\n",
    "\n",
    "# get model predictions\n",
    "top_10_tokens = get_top_k_tokens(icl_prompt, models_dict[model_name])\n",
    "\n",
    "# inspect results\n",
    "print(\"MODEL:\", model_name)\n",
    "print(f\"PROMPT:\\n{icl_prompt}\")\n",
    "print(\"ANSWER:\", answer)\n",
    "print(\"TOP 10 TOKENS:\", top_10_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "model_name = \"gpt3\"\n",
    "num_demos = 1\n",
    "qa=False\n",
    "instruction=\"Translate the following word to french.\"\n",
    "questions, answers = get_demonstrations_world_capitals()\n",
    "\n",
    "# construct ICL prompt\n",
    "questions, answers = sample_and_shuffle_demonstrations(\n",
    "    questions, answers, num_demos\n",
    ")\n",
    "icl_prompt, answer = construct_icl_prompt(\n",
    "    questions, \n",
    "    answers, \n",
    "    qa_template=qa,\n",
    "    instruction=instruction\n",
    ")\n",
    "\n",
    "# get model predictions\n",
    "top_10_tokens = get_top_k_tokens(icl_prompt, models_dict[model_name])\n",
    "\n",
    "# inspect results\n",
    "print(\"MODEL:\", model_name)\n",
    "print(f\"PROMPT:\\n{icl_prompt}\")\n",
    "print(\"ANSWER:\", answer)\n",
    "print(\"TOP 10 TOKENS:\", top_10_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load Llama-3.2-1B-Instruct \n",
    "llama_instruct_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "models_dict[\"llama_instruct\"].append(\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        llama_instruct_name, \n",
    "        device_map=DEVICE, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "    )\n",
    ")\n",
    "models_dict[\"llama_instruct\"].append(\n",
    "    AutoTokenizer.from_pretrained(\n",
    "        llama_instruct_name, padding_side=\"left\"\n",
    "        )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "model_name = \"gpt3\"\n",
    "num_demos = 1\n",
    "qa=True\n",
    "instruction=\"Solve the following task.\"\n",
    "questions, answers = get_demonstrations_world_capitals()\n",
    "\n",
    "# construct ICL prompt\n",
    "questions, answers = sample_and_shuffle_demonstrations(\n",
    "    questions, answers, num_demos\n",
    ")\n",
    "icl_prompt, answer = construct_icl_prompt(\n",
    "    questions, \n",
    "    answers, \n",
    "    qa_template=qa,\n",
    "    instruction=instruction\n",
    ")\n",
    "\n",
    "# get model predictions\n",
    "top_10_tokens = get_top_k_tokens(icl_prompt, models_dict[model_name])\n",
    "\n",
    "# inspect results\n",
    "print(\"MODEL:\", model_name)\n",
    "print(f\"PROMPT:\\n{icl_prompt}\")\n",
    "print(\"ANSWER:\", answer)\n",
    "print(\"TOP 10 TOKENS:\", top_10_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Longer Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(\n",
    "        prompt, \n",
    "        model_tokenizer, \n",
    "        num_tokens=10,\n",
    "        verbose=False,\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Returns string constructed with sequence of num_tokens tokens predicted by\n",
    "    given model using greedy decoding on given prompt.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt : str\n",
    "        Prompt to be used for generation.\n",
    "    model_tokenizer : tuple\n",
    "        Tuple of model and tokenizer\n",
    "    num_tokens : int\n",
    "        Number of tokens to be generated.\n",
    "    num_tokens : bool\n",
    "        Flag for printing tokens as they are generated.\n",
    "    \"\"\"\n",
    "\n",
    "    # unpacking\n",
    "    model = model_tokenizer[MODEL]\n",
    "    tokenizer = model_tokenizer[TOKENIZER]\n",
    "\n",
    "    final_str = \"\"\n",
    "    \n",
    "    ### WRITE YOUR CODE HERE ### \n",
    "\n",
    "    return final_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your function\n",
    "prompt = \"Hello! \"\n",
    "model_name = \"gpt3\"\n",
    "# generate text\n",
    "generated_text = generate(\n",
    "    prompt, \n",
    "    models_dict[model_name], \n",
    ")\n",
    "print(\"GENERATED TEXT:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "model_name = \"llama_instruct\"\n",
    "num_tokens = 20\n",
    "\n",
    "# set prompt\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# generate answer\n",
    "generated_str = generate(\n",
    "    prompt, \n",
    "    models_dict[model_name], \n",
    "    num_tokens,\n",
    ")\n",
    "print(\"GENERATED STR:\\n\")\n",
    "print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings \n",
    "model_name = \"llama_instruct\"\n",
    "num_tokens = 10\n",
    "\n",
    "# set prompt\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# unpack model and tokenizer\n",
    "model = models_dict[model_name][MODEL]\n",
    "tokenizer = models_dict[model_name][TOKENIZER]\n",
    "\n",
    "# we need to tokenize the prompt ourselves\n",
    "tokenized_prompt = tokenizer(\n",
    "    prompt, \n",
    "    return_tensors=\"pt\"\n",
    ").to(DEVICE)\n",
    "\n",
    "# generate answer with model's generate function\n",
    "generated_ids = model.generate(\n",
    "    **tokenized_prompt,\n",
    "    max_new_tokens=num_tokens,\n",
    "    )\n",
    "\n",
    "# inspect results\n",
    "# note this output already includes the prompt\n",
    "print(\"\\nOUTPUT:\\n\")\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to construct chat history\n",
    "def construct_chat_prompt(\n",
    "        new_prompt: str, \n",
    "        chat_history: str = \"\", \n",
    "        system_prompt: str = None,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Constructs prompt for chatting with model. \n",
    "\n",
    "    Args:\n",
    "        new_prompt (str): new user entry in conversation\n",
    "        chat_history (str): all of the conversation so far\n",
    "        system_prompt (str): system prompt to give model initial instructions\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\n",
    "    if system_prompt is not None:\n",
    "        prompt = f\"{system_prompt}\\n\\n\"\n",
    "\n",
    "    return prompt + chat_history + new_prompt + \"\\n\\n<ASSISTANT>\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings \n",
    "model_name = \"llama_instruct\"\n",
    "num_tokens = 20\n",
    "\n",
    "# set up a system prompt\n",
    "system_prompt = \"\"\"You are a helpful assistant. You will answer the user's questions in a friendly and informative manner.\" \n",
    "\n",
    "<USER>\"\"\"\n",
    "\n",
    "# set up new dialogue entry\n",
    "dialogue_entry = \"Hello? Who are you?\"\n",
    "\n",
    "# construct chat prompt\n",
    "prompt = construct_chat_prompt(\n",
    "    new_prompt=dialogue_entry, \n",
    "    chat_history=\"\", \n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "# unpack\n",
    "model = models_dict[model_name][MODEL]\n",
    "tokenizer = models_dict[model_name][TOKENIZER]\n",
    "\n",
    "# we need to tokenize the prompt ourselves\n",
    "tokenized_prompt = tokenizer(\n",
    "    prompt, \n",
    "    return_tensors=\"pt\"\n",
    ").to(DEVICE)\n",
    "\n",
    "# generate model response\n",
    "generated_ids = model.generate(\n",
    "    **tokenized_prompt,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=num_tokens,\n",
    "    )\n",
    "\n",
    "# inspect results\n",
    "print(\"\\nOUTPUT:\\n\")\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
