\documentclass[t]{beamer}

% packages
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{numprint}
\usepackage{amsxtra}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{amssymb, amstext, amsmath}
\usepackage{fancyhdr}

% macros
\input{../macros}

% new commands
\newcommand\op[1]{\operatorname{#1}}

% choose how your presentation looks.
% for more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html

\mode<presentation>
{%
	\usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
	\usecolortheme{default} % or try albatross, beaver, crane, ...
	\usefonttheme{default}  % or try serif, structurebold, ...
	\setbeamertemplate{navigation symbols}{}
	\setbeamertemplate{caption}[numbered]
	% For a numbered table of contents
	\setbeamertemplate{section in toc}[sections numbered] 
	% For slide numbers
	\addtobeamertemplate{navigation symbols}{}{%
	\usebeamerfont{footline}
	\usebeamercolor[fg]{footline}
	\hspace{1em}
	\insertframenumber%/\inserttotalframenumber
	}
} 

%% so table of content appears before each section, highlighting what's next
%\AtBeginSection[]
%{%
%	\setbeamercolor{section in toc shaded}{fg=structure}
%	\begin{frame}<beamer>
%	  \frametitle{Outline}
%	  \tableofcontents[currentsection]
%	\end{frame}
%}

% adds title slides for each section
\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\Huge\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

\title[Write your short title here]{Advanced Methods in Text Analytics}
\subtitle{Exercise 6: Transformers - Part 2}
\author{Daniel Ruffinelli}
\institute{University of Mannheim}
\date{FSS 2025}

\begin{document}

% no "Figure X" prefix in image captions when using the figure environment
\setbeamertemplate{caption}{\raggedright\insertcaption\par}

\begin{frame}
    \titlepage{}
\end{frame}

\begin{frame}{The Residual Stream}{Context}
    \begin{itemize}
        \item In this task, we take a closer look at both the forward and
              backward pass of a
              \href{https://arxiv.org/pdf/1706.03762}{\underline{transformer}}
              layer (see image from original paper below).
        \item By \emph{transformer layer}, we mean the combination of a
              multi-head self-attention followed by a feed-forward neural
              network, each with layer normalization applied after it, and each
              with a residual connection around them.
    \end{itemize}
    \begin{center}
        \includegraphics[scale=0.4]{img/transformer_layer.png}
    \end{center}
\end{frame}

\begin{frame}{The Residual Stream}{Question a)}
    \begin{itemize}
        \item Let $\op{SA}$ be a self-attention layer parameterized by
              $\mW^Q,\mW^K,\mW^V$, and let $\mX\in \bb{R}^{n\times d}$ be the
              $d$-dimensional tokens from an input sequence of length $n$.
        \item Give a formal expression for the output of $SA$ given $\mX$ as
              input.
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Answer a)}
    \begin{itemize}
        \item We have:
              \begin{align}
                  \op{SA}(\mX) & = \op{softmax}\left(\frac{\mX\mW^Q(\mX\mW^K)^T}{\sqrt{d}}\right)\mX\mW^V
              \end{align}
              where the softmax function is applied row-wise and
              $\mX\mW^Q = \mQ$ is referred to as the query matrix,
              $\mX\mW^K = \mK$ is the key matrix, and $\mX\mW^V = \mV$ the value
              matrix.
        \item Recall that the factor $\frac{1}{\sqrt{d}}$ is used for numerical
              stability (scaled dot-product attention).
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Question b)}
    \begin{itemize}
        \item Let $\op{FNN}$ be the feed-forward neural network applied after
              $\op{SA}$ in a transformer layer, and assume it projects the
              vectors in input $\mX$ to a $m$-dimensional space and then back
              down to a $d$-dimensional space.
        \item Give a formal expression for the output of $\op{FNN}$ given $\mX$
              as input.
        \item Assume a $ReLU$ activation function is used after the first
              projection, and make sure to specify the size of the parameters
              in $\op{FNN}$.
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Answer b)}
    \begin{itemize}
        \item We have:
              \begin{align}
                  \op{FNN}(\mX) & = \text{ReLU}\left((\mX\mW_1 + \vb_1)\right)\mW_2 + \vb_2
              \end{align}
              where parameters are $\mW_1\in\bb{R}^{d\times m}$,
              $\vb_1\in\bb{R}^m$, $\mW_2\in\bb{R}^{m\times d}$, and
              $\vb_2\in\bb{R}^d$.
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Question c)}
    \begin{itemize}
        \item Using $\op{SA}$ and $\op{FNN}$, give an expression for the output
              of a transformer layer $\op{TF}$ given $\mX$ as input.
        \item Use $\op{LN_1}$ and ${LN_2}$ for layer normalization after
              $\op{SA}$ and $\op{FNN}$, respectively.
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Answer c)}
    \begin{itemize}
        \item We do this in stages. First, we focus on $\op{SA}$, its
              corresponding residual connection and layer normalization
              $\op{LN_1}$.
              We have:
              \begin{align}
                  \op{\mO_1} & = \op{LN_1}(\op{SA}(\mX) + \mX)
              \end{align}
              Then, we focus on $\op{FNN}$, its corresponding residual
              connection and layer normalization area applied.
              That is:
              \begin{align}
                  \op{\mO_2} & = \op{LN_2}(\op{FNN}(\mO_1) + \mO_1)
              \end{align}
              All together, we have:
              \begin{align}
                  \op{TF}(\mX) & = \op{LN_2}(\op{FNN}(\op{LN_1}(\op{SA}(\mX) + \mX)) + \op{LN_1}(\op{SA}(\mX) + \mX)).
              \end{align}
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Question d)}
    \begin{itemize}
        \item What do the layer normalization operators $LN_i$ do?
        \item And could we use them in a different part of the transformer
              layer?
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Answer d)}
    \begin{itemize}
        \item Layer normalization is used to normalize the vectors it is given
              as input.
        \item Concretely, given a vector $\vx\in\bb{R}^d$, the layer
              normalization operation is given by:
              \begin{align}
                  \op{LN}(\vx) & = \frac{\vx - \mu}{\sigma} \gamma + \beta
              \end{align}
              where $\gamma$ and $\beta$ are learned parameters.
        \item This operation can be seen as a form of data centering and is
              commonly used so keep training stable, sometimes at the cost of
              performance on some downtream tasks.
        \item It can be applied anywhere in a network/transformer.
        \item In fact, a common variant is the \emph{pre-norm} transformer,
              which applies layer normalization \emph{before} each of $\op{SA}$
              and $\op{FNN}$.
        \item Typically, this variant also includes a single final layer
              normalization operation applied only to the output of the last
              transformer layer.
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Question e)}
    \begin{itemize}
        \item Let us now focus on the backward pass.
        \item Recall that during backpropagation, we apply the chain rule of
              calculus to compute the gradients of all parameterized operators
              used in the forward pass, usually w.r.t. some loss function $L$.
        \item In the context of some transformer layer $\op{TF_i}$, these
              operators are $\op{SA},\op{FNN},\op{LN_1}$ and $\op{LN_2}$.
        \item Give a formal expression for the gradient needed to update
              parameters $\theta_{SA}$ of operator $\op{SA}$ w.r.t.\ some loss
              function $L$.
        \item Omit the use of layer normalization and residual connections for
              simplicity.
        \item Indicate which part of the expression is ``received'' from the
              $\op{FNN}$ layer above during backpropagation.
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Answer e) (1)}
    \begin{itemize}
        \item To update parameters $\theta_{SA}$ of $\op{SA}$ w.r.t.\ some loss
              function $L$ during training, we need:
              \begin{align}\label{eq:weight_gradient}
                  \frac{\partial L}{\partial \theta_{SA}} = \frac{\partial L}{\partial \op{SA}(\mH)}\frac{\partial \op{SA}(\mH)}{\partial \theta_{SA}},
              \end{align}
              where $\mH$ is the input to $\op{SA}$, which in turn is the output
              of the previous transformer layer $\op{TF_{i-1}}$.
        \item The term $\frac{\partial L}{\partial \op{SA}(\mH)}$ is the partial
              gradient received during backpropagation from the upper layer, the
              $\op{FNN}$ operator in the context of a transformer layer.
        \item Let's try to visualize this in the next slide.
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Answer e) (2)}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[scale=0.4]{img/transformer_layer.png}
                \\ \vspace{1cm}Forward Pass
            \end{center}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[scale=0.4]{img/transformer_layer.png}
                \\ \vspace{1cm}Backward Pass
            \end{center}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{The Residual Stream}{Question f)}
    \begin{itemize}
        \item Following your previous answer, what is the gradient that is
              passed down to lower layers, e.g.\ the lower transformer layer
              $\op{TF_{i-1}}$?
        \item Give a formal expression for this gradient and briefly describe
              why this gradient is needed during training.
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Answer f)}
    \begin{itemize}
        \item Just as we received a gradient from the upper layer during
              training, we pass down the following gradient to the lower
              transformer layer $\op{TF_{i-1}}$ during backpropagation:
              \begin{align}\label{eq:input_gradient}
                  \frac{\partial L}{\partial \mH} = \frac{\partial L}{\partial \op{SA}(\mH)}\frac{\partial \op{SA}(\mH)}{\partial \mH}.
              \end{align}
        \item This is the gradient of the output of $\op{TF_{i-1}}$ w.r.t. $L$,
              and is needed to compute the weight updates of the operators in
              lower layers that contribute to the computation of the input to
              $\op{TF_i}$, i.e.\ $\mH$.
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Question g)}
    \begin{itemize}
        \item Now rewrite your previous answer while considering the residual
              connection around $\op{SA}$.
        \item Simplify the resulting expression as much as possible with the
              goal of answering the question: what happens during training to
              the gradient that we received from higher layers as it's passed
              through an $\op{SA}$ operator that uses residual connection?
        \item \textbf{Hint:} No need to compute any gradients, but do use the
              fact that the gradient is a linear operator.
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Answer g) (1)}
    \begin{itemize}
        \item Let $\mO_1$ be the output of $\op{SA}$ with a residual connection
              around it, i.e.\ $\mO_1 = \op{SA}(\mH) + \mH$.
        \item Then, the gradient ``received'' from the upper layer is
              $\frac{\partial L}{\partial \mO_1}$.
        \item Eq.~\ref{eq:input_gradient} can then be rewritten as follows:
              \begin{align*}
                  \frac{\partial L}{\partial \mH} & = \frac{\partial L}{\partial \mO_1}\frac{\partial \mO_1}{\partial \mH}                                                         \\
                                                  & = \frac{\partial L}{\partial \mO_1}\frac{\partial (\op{SA}(\mH) + \mH)}{\partial \mH}                                          \\
                                                  & = \frac{\partial L}{\partial \mO_1}\left(\frac{\partial \op{SA}(\mH)}{\partial \mH} + \frac{\partial \mH}{\partial \mH}\right) \\
                                                  & = \frac{\partial L}{\partial \mO_1}\frac{\partial \op{SA}(\mH)}{\partial \mH} + \frac{\partial L}{\partial \mO_1}.
              \end{align*}
        \item In other words, when using residual connections, the gradient
              $\frac{\partial L}{\partial \mO_1}$ we receive from higher layers
              during backpropagation is passed down to lower layers by making a
              modification via the addition operation.
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Answer g) (2)}
    \begin{itemize}
        \item Note that without the residual connection, the gradient we pass
              down would be modified by the multiplying factor
              $\frac{\partial \op{SA}(\mH)}{\partial \mH}$, and the additive
              term would not exist.
        \item Since such a multiplying factor can quickly reduce the size of the
              gradient that is passed backward during training, the residual
              connection was designed by
              \href{https://arxiv.org/abs/1512.03385}{\underline{He et al. 2015}}
              so that gradients can be more safely passed through an operator
              without it having a severe impact on those gradients.
        \item The name ``residual'' connection comes from the fact that, due to
              this change to prevent gradients from vanishing during training,
              the function $f(\mX) = \op{OP}(\mX)$ computed by some operator
              becomes $f(\mX) = \op{OP}(\mX) + \mX$.
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Answer g) (3)}
    \begin{itemize}
        \item This means that we are learning to apply a suitable \emph{difference}
              $\op{OP}(\mX)$ that is added to given input $\mX$.
        \item In the context of a transformer layer, $\mX$ is a set of
              contextualized representations, so what each transformer layer
              learns is to add something to it \emph{if needed}.
        \item If $\mX$ is already suitable for the task, then a layer may make
              small modifications to it.
        \item This suggests that the main flow of information is coming from
              $\mX$, and not from $\op{OP}(\mX)$, or in other words, from the
              residual connections and not from the transformer layers.
        \item This perspective is known as the ``residual stream'' view of
              transformers, illustrated in the following image from Jurasfky and
              Martin (2024), Section 10.4.
    \end{itemize}
\end{frame}

\begin{frame}{The Residual Stream}{Answer g) (4)}
    \begin{center}
        \includegraphics[scale=0.27]{img/residual_stream.png}
        \\ The Residual Stream. Main information flows through residual
        connections.
    \end{center}
\end{frame}

\begin{frame}{Attention Heads are Independent}{Question a)}
    \begin{itemize}
        \item Let $\op{MHA}$ be a multi-head attention layer, $\op{SA}_i$ its
              $i$-th self-attention layer and $\mX\in \bb{R}^{n\times d}$ the
              $d$-dimensional tokens from an input sequence of length $n$.
        \item Give a formal expression for the output of $\op{MHA}$ as a
              function of its $k$ attention heads $SA_i$, i.e. $1 <= i <= k$,
              given $\mX$ as input.
        \item As usual, make sure to formally define all components in your
              expression.
        \item How is $\op{MHA}$ parameterized?
    \end{itemize}
\end{frame}

\begin{frame}{Attention Heads are Independent}{Answer a)}
    \begin{itemize}
        \item We have:
              \begin{align}
                  \op{MHA}(\mX) & = \left(SA_1(\mX) \oplus SA_2(\mX) \oplus \ldots \oplus SA_k(\mX)\right)\mW^O + \mX,
              \end{align}
              where $\oplus$ is the concatenation operator,
              $\mW_O\in\bb{R}^{kd\times d}$ is a parameter matrix that projects
              the concatenated vector back to $d$-dimensional residual space,
              and the additive term is the residual connection around MHA.
        \item Note that the size of $\mW^O$ is, indirectly, a design choice, as
              it depends on the number of attention heads (in this case k) and
              the size of tokens (in this case $d$).
    \end{itemize}
\end{frame}

\begin{frame}{Attention Heads are Independent}{Question b)}
    \begin{itemize}
        \item Assume MHA takes as input the output $\mO^i$ of each
              self-attention head
              $\op{SA}_i(\mX)$, i.e.\ $\mO^i = \op{SA}_i(\mX)$.
        \item Give a formal expression for the computations needed to compute a
              single component of the output matrix of $\op{MHA}$ as a function
              of each $\mO^i$.
    \end{itemize}
\end{frame}

\begin{frame}{Attention Heads are Independent}{Answer b) (1)}
    \begin{itemize}
        \item The operation in question here is the projection to residual space
              done by $\mW^O$.
        \item As defined in the task description, let
              $\mO^i\in\bb{R}^{n\times d}$ be the output of $\op{SA_i}$.
        \item After concatenating the output of
              each attention head, we have $\mO\in\bb{R}^{n\times kd}$ where $k$
              is the number of attention heads.
        \item That is, we have the following
              block matrix:
              \begin{align}
                  \mO & = \left[\mO^1 \mO^2 \ldots \mO^k\right].
              \end{align}
        \item Now, note that when computing each element $ij$ of $\mO\mW^O$,
              i.e.\ $[\mO\mW^O]_{ij}$, we compute the following dot product:
              \begin{align}
                  [\mO\mW^O]_{ij} & = \sum_{h=1}^{kd} \mO_{ih}\mW^O_{hj}.
              \end{align}
    \end{itemize}
\end{frame}

\begin{frame}{Attention Heads are Independent}{Answer b) (2)}
    \begin{itemize}
        \item We can rewrite this dot product to make use of the $\mO^i$
              components of $\mO$ more explicitly, as follows:
              {\small
              \begin{align}
                  [\mO\mW^O]_{ij} & = \sum_{h=1}^{d} \mO_{ih}\mW^O_{hj} + \sum_{h=d+1}^{2d} \mO_{ih}\mW^O_{hj} + \ldots + \sum_{h=(k-1)d+1}^{kd} \mO_{ih}\mW^O_{hj}         \\
                                  & = \sum_{h=1}^{d} \mO^1_{ih}\mW^{O_1}_{hj} + \sum_{h=1}^{d} \mO^2_{ih}\mW^{O_2}_{hj} + \ldots + \sum_{h=1}^{d} \mO^k_{ih}\mW^{O_k}_{hj},
              \end{align}
              }
              where we now defined $\mW^{O_i}\in\bb{R}^{d\times d}$ as the
              $i$-th block of $\mW^O$ that corresponds to $\mO^i$.
        \item That is, we can also see $\mW^O$ as the following block matrix:
              \begin{align}
                  \mW^O & = \left[\mW^{O_1} \mW^{O_2} \ldots \mW^{O_k}\right]^\top.
              \end{align}
    \end{itemize}
\end{frame}

\begin{frame}{Attention Heads are Independent}{Question c)}
    \begin{itemize}
        \item Using the intuition built in the previous subtask, show that the
              output of the multi-head attention layer $\op{MHA}$ can be
              expressed as the following linear combination:
              \begin{align}
                  \op{MHA}(\mX) & = \sum_{i=1}^{k} \mO^i\mM^i + \mX,
              \end{align}
              and make sure to define exactly what each $\mM^i$ is.
        \item What does this rewriting of the multi-head attention layer imply
              about the operations computed by each attention head?
    \end{itemize}
\end{frame}

\begin{frame}{Attention Heads are Independent}{Answer c) (1)}
    \begin{itemize}
        \item From the solution to the previous subtasks, it follows that we can
              represent the general operation of $\op{MHA}$ as:
              \begin{align}
                  \op{MHA}(\mX) & = \left[\mO^1\mO^2\ldots\mO^k\right] \begin{bmatrix}
                                                                           \mW^{O_1} \\
                                                                           \mW^{O_2} \\
                                                                           \ldots    \\
                                                                           \mW^{O_k}
                                                                       \end{bmatrix}
                  + \mX.
              \end{align}
        \item And being as this is a matrix multiplication of block matrices, we
              can write this as the following linear combination:
              \begin{align}
                  \op{MHA}(\mX) & = \sum_{i=1}^{k} \mO^i\mW^{O_i} + \mX,
              \end{align}
              where $\mM^i = \mW^{O_i}$ as desired.
    \end{itemize}
\end{frame}

\begin{frame}{Attention Heads are Independent}{Answer c) (2)}
    \begin{itemize}
        \item This equivalence shows that the output of the multi-head attention
              layer $\op{MHA}$ can be expressed as a linear combination of the
              outputs of each attention head $\op{SA}_i(\mX)$, where the weights
              of that linear combination are learned by the layer.
        \item Thus, each (weighted) attention head contributes to the
              learned representations by \emph{independently adding} to the 
              representations in the residual space.
        \item Indeed, the field of interpretability has identified that certain
              attention heads learn specific roles that completely differ from 
              other attention heads.
        \item For more details, see 
        \href{https://transformer-circuits.pub/2021/framework/index.html}{here}.
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Context}
    \begin{itemize}
        \item In this exercise, we have a quick look at how to implement a
              language model with transformers using
              \href{https://pytorch.org/}{PyTorch}.
        \item As before with our FNN-based and RNN-based language models, we
              will need to define a class for the model, a training loop and an
              evaluation loop.
        \item In addition, we also need to implement positional encodings.
        \item We use the ones used by the original transformer architecture.
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Question a)}
    \begin{itemize}
        \item Read the code for the class \emph{PositionalEncoding} to make sure
              it's correct w.r.t.\ how these positional embeddings are defined.
        \item Then check documentation for PyTorch's
              \href{https://pytorch.org/docs/stable/generated/torch.nn.Module.html}{Module}.
        \item What does the method \emph{register\_buffer} do?
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Answer a)}
    \begin{itemize}
        \item Storing models during or after training is important to later be
              able to use them, e.g.\ for inference, fine-tuning or to continue
              training.
        \item PyTorch models have a state dictionary (state\_dict) that is
              usually stored on disk in so-called \emph{checkpoint} files.
        \item It's important that these files contain all the necessary
              information so that we may later be able to use the model.
        \item \emph{register\_buffer} is used to define ``parameters'' of a
              model that need to be stored in its state\_dict, but that aren't
              actually parameters in the learning sense.
        \item That is, they are not changed during training.
        \item Since we are implementing a non-parameterized positional encoding,
              we register these embeddings as buffers, because we later need
              them for using the model.
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Question b)}
    \begin{itemize}
        \item Read the code for the TransformerModel class. What kind of
              architecture does it use? Encoder-decoder? Encoder-only?
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Answer b) (1)}
    \begin{itemize}
        \item The answer is not straightforward.
        \item If we look at the class from PyTorch that we use, it's the
              $\op{TransformerEncoder}$ class.
        \item But, as we'll see later, we apply a mask to our attention scores,
              which have an impact on the type of model we are using, so we'll
              come back to this in the next few questions.
        \item In addition, the model does not have cross-attention (attention
              between decoder and the outputs of the encoder), so it is
              definitely not an encoder-decoder model either.
        \item The code does have a component called a decoder, but what is that
              exactly?
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Answer b) (2)}
    \begin{itemize}
        \item It's a linear layer that projects down to a vector space the size
              of our vocabulary, so while common, this naming convention can be
              confusing, as this decoding step refers to decoding in the
              tokenization sense.
        \item This projection, in combination with a softmax, will be used to
              produce probabilities over our vocabulary in order to predict the
              next token (if we construct the training examples correctly).
        \item But why isn't the softmax function there then?
              Well, as in our previous coding exercise, we will use
              $\op{CrossEntropyLoss}$, which includes the softmax function, so
              we need our model to output logits.
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Question c)}
    \begin{itemize}
        \item What is the role of the function
              \emph{\_generate\_square\_subsequent\_mask}? Read the documentation
              for the
              \href{https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html}{Transformer Class}
              to find out more.
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Answer c)}
    \begin{itemize}
        \item This function is used to construct an attention mask.
              Masking is the process by which we ``block'' some entries in a
              tensor.
        \item In this context, the forward pass of the $\op{TransformerEncoder}$
              takes a mask as input along side the input sequence.
        \item This mask is added to the matrix of attention \emph{scores} in the
              self-attention layer.
        \item If scores are \emph{-Inf}, their attention weight (via softmax) is
              zero.
        \item Entry $ij$ in this matrix can be seen as how much attention input
              token $i$ pays to $j$.
        \item Thus, the mask is a matrix with zeros in the lower triangular, and
              the rest \emph{-Inf}.
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Question d)}
    \begin{itemize}
        \item What kind of language model is it? Causal or masked? Why?
              \textbf{Hint:} What does the function triu do?
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Answer d)}
    \begin{itemize}
        \item Given the attention mask we use, as explained in the answer above,
              this is a causal language model.
        \item Note that the use of this \emph{causal mask}, in combination with
              the lack of cross-attention, means we have a decoder-only model,
              despite using a $\op{TransformerEncoder}$ class.
        \item It is common to implement such a CLM using
              $\op{TransformerEncoder}$.
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Question e)}
    \begin{itemize}
        \item Complete the \emph{forward} function of your transformer.
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Answer e)}
    \begin{itemize}
        \item See Jupyter notebook.
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Question f)}
    \begin{itemize}
        \item We can train this transformer in the same way that we trained our
              RNN in our last coding exercise: with \emph{teacher forcing}.
        \item So, we will use the same methods for preprocessing the data,
              creating the batches and training the model.
        \item Note that the training method was modified slightly to accomodate
              this model's forward function (no need to pass the hidden state as
              with RNNs).
        \item Use the given code to train the transformer on the
              \emph{Shakespeare} data.
        \item What perplexity do we get on validation data?
              Is it higher or lower than when using an RNN as before?
        \item Is the task comparable w.r.t.\ that case? Discuss.
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Answer f) (1)}
    \begin{itemize}
        \item We get validation perplexity (PPL) of about 500, which is somewhat
              higher than with the RNN, which was about 400.
        \item Recall that PPL can be interpreted as proportional to the
              vocabulary size, which is about 30K.
        \item In addition, and as discussed in previous tutorials, it can be
              interpreted as the average \emph{branching factor}, i.e.\ the
              average number of possible next words after a given one.
        \item The PPL values from the RNN and this transformer are inded
              comparable,
              as both models are evaluated on the same task using the same
              validation data (it's virtually the same code base).
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Answer f) (2)}
    \begin{itemize}
        \item As for basic resources, they are also comparable.
              The embedding size is the same as the one used by the RNN, and
              both use dropout with default values as the only form of
              regularization.
        \item It's therefore likely can this slighly lower performance is due to
              the stronger inductive bias in the transformer model, i.e.\ it's
              overfitting.
        \item It would be nice to try to use a transformer with lower number of
              attention heads, or lower number of layers.
        \item And indeed, such tests are important to do on validation data
              before deciding which model will be used on test data or
              real-world data.
        \item We look at this further in the next question.
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Question f)}
    \begin{itemize}
        \item Can you modify the model so it performs better? Consider using
              different embedding sizes, different number of layers, etc.
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Answer f) (1)}
    \begin{itemize}
        \item See Jupyter notebook for details.
        \item When training longer, for 20 epochs, we see the transformer
              achieving a PPL of about 400, i.e.\ it achieves the same
              performance achieved by the RNN in a single epoch.
        \item But when training the RNN for 20 epochs, it gets a PPL of about
              350, so while smaller, there is still a performance gap in favor
              of the RNN.
        \item W.r.t.\ changing model depth, we get slightly lower performance
              with a single layer, and a marginal improvement with 3 layers.
        \item But when using 6 layers performance drops, suggesting overfitting.
    \end{itemize}
\end{frame}

\begin{frame}{Language Models With Transformers}{Answer f) (2)}
    \begin{itemize}
        \item Finally, we could use a
              \href{https://machinelearningmastery.com/using-learning-rate-schedule-in-pytorch-training/}{\underline{learning rate scheduler}}
              to get an increase in performance, as the learning rate is often a
              crucial hyperparameter in deep learning models, so stuff like
              a \href{https://www.baeldung.com/cs/learning-rate-warm-up}{\underline{learning rate warmup}}
              are common practice when training LLMs.
        \item In general, the transformer does not quite achieve the performance
              of the RNN in our tests, but it's possible that with other
              modifications we can get our transformer to performn better, e.g.\
              using smaller hidden sizes, a different number of attention heads,
              etc.
        \item And what about model stability? I.e.\ how large is
              the variance of PPL when training the same model multiple times?
        \item Running the models in these tests multiple times suggests they are
              not quite stable, i.e.\ it's possible that the
              transformer and the RNN perform similarly when comparing the mean
              and variance of PPL over, say, 10 different runs of each model.
    \end{itemize}
\end{frame}

\end{document}
