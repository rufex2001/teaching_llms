{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Methods in Text Analytics\n",
    "# Exercise 6: Transformers - Part 2\n",
    "### Daniel Ruffinelli\n",
    "## FSS 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device to \"cpu\" if you don't have a GPU\n",
    "DEVICE=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "            )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1).to(DEVICE)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions (b) to (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, d_model, nhead, d_hid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, d_hid, dropout\n",
    "            )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layers, nlayers\n",
    "            )\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "    # mask for language modeling\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        # change all the zeros to negative infinity and all the ones to zeros \n",
    "        # as follows:\n",
    "        mask = mask.float().masked_fill(\n",
    "            mask == 0, float('-inf')\n",
    "            ).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions (e) to (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our whitespace tokenizer that removes punctuation\n",
    "def tokenize(text):\n",
    "    \"\"\" \n",
    "    Given text, returns all words separated by white space after removing all\n",
    "    punctuation, except full stops.\n",
    "\n",
    "    Args:\n",
    "        text: string with text to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list of tokens\n",
    "    \"\"\"\n",
    "\n",
    "    import string\n",
    "\n",
    "    # separate punctuation symbols with whitespaces\n",
    "    for symbol in string.punctuation:\n",
    "        text = text.replace(symbol, \" \" + symbol + \" \")\n",
    "    text_split = text.split()\n",
    "\n",
    "    return text_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 29245\n"
     ]
    }
   ],
   "source": [
    "# as before, we work with embeddings now, not just strings\n",
    "from collections import defaultdict as ddict\n",
    "\n",
    "# these are our splits\n",
    "shakespeare_splits = {\n",
    "    \"train\": \"shakespeare_train.txt\", \n",
    "    \"valid\": \"shakespeare_valid.txt\", \n",
    "    \"text\": \"shakespeare_test.txt\"\n",
    "}\n",
    "\n",
    "# we create a vocabulary dict of the form {token: ID}\n",
    "shakespeare_vocab = {}\n",
    "for text_file in shakespeare_splits.values():\n",
    "    with open(text_file) as f:\n",
    "        split_text = f.read()\n",
    "        tokenized_split = tokenize(split_text)\n",
    "        for token in tokenized_split:\n",
    "            if token not in shakespeare_vocab:\n",
    "                shakespeare_vocab[token] = len(shakespeare_vocab)\n",
    "# we add the padding symbol to our vocabulary\n",
    "shakespeare_vocab[\"<s>\"] = len(shakespeare_vocab)\n",
    "print(\"Size of vocabulary:\", len(shakespeare_vocab))\n",
    "\n",
    "# we turn our splits into sequences of token IDs\n",
    "shakespeare_splits_ids = ddict(list)\n",
    "for split_id, split_file in shakespeare_splits.items():\n",
    "    with open(split_file) as f:\n",
    "            tokenized_split = tokenize(f.read())\n",
    "    for token in tokenized_split:\n",
    "        shakespeare_splits_ids[split_id].append(shakespeare_vocab[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our torch dataset object\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SelfSupervisedTextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenized_text, example_length):\n",
    "        \"\"\"\n",
    "        Dataset to process text examples constructed with self-supervision.\n",
    "\n",
    "        Args:\n",
    "            tokenized_text (string): list of tokens to construct examples\n",
    "            example_length (int): length of inputs strings for model\n",
    "        \"\"\"\n",
    "\n",
    "        # we divide tokenized text into subsequences of (equal) example_length\n",
    "        # we ignore leftover tokens at the end\n",
    "        self._examples = []\n",
    "\n",
    "        for i in range(0, len(tokenized_text), example_length):\n",
    "            self._examples.append(tokenized_text[i:i + example_length])\n",
    "        if len(self._examples[-1]) < example_length:\n",
    "               self._examples.pop()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self._examples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.SelfSupervisedTextDataset object at 0x7fe3c81b8340>\n"
     ]
    }
   ],
   "source": [
    "# create shakespeare dataset\n",
    "max_input_length = 64\n",
    "training_dataset = SelfSupervisedTextDataset(shakespeare_splits_ids[\"train\"], \n",
    "                                             max_input_length)\n",
    "print(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our collate function\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Function to construct labeled example from given batch.\n",
    "\n",
    "    Args:\n",
    "        batch (tensor): tensor of size batch_size x sentence_length with tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # we create two lists for our training examples: inputs and corresponding \n",
    "    # targets    \n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for example in batch:\n",
    "        inputs.append(torch.tensor(example[:-1]))\n",
    "        targets.append(torch.tensor(example[1:]))\n",
    "\n",
    "    return torch.stack(inputs, dim=0), torch.stack(targets, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fe3c81b3fd0>\n"
     ]
    }
   ],
   "source": [
    "# our dataloader for training data\n",
    "batch_size = 128\n",
    "training_dataloader = DataLoader(training_dataset, \n",
    "                                 collate_fn=collate_fn,\n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=True, \n",
    "                                 num_workers=0)\n",
    "print(training_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our training loop\n",
    "import time, math\n",
    "\n",
    "def train(\n",
    "        model, dataloader, num_tokens, num_epochs=10, print_batch_stats=False\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "\n",
    "    Args:\n",
    "        model: some LM implemented in PyTorch\n",
    "        dataloader: dataloader that returns sentences as examples\n",
    "        num_epochs (int): number of epochs to train \n",
    "    \"\"\"\n",
    "\n",
    "    # set training hyperparameters\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    learning_rate = 0.1\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # we iterate over epochs\n",
    "    num_batches = len(dataloader)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.\n",
    "        start_time = time.time()\n",
    "\n",
    "        # we iterate over batches\n",
    "        for batch_num, (inputs, targets) in enumerate(dataloader):\n",
    "\n",
    "            # move inputs and targets to device\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "\n",
    "            # forward pass\n",
    "            output = model(inputs)\n",
    "            loss_value = loss_fn(output.view(-1, num_tokens), targets.view(-1))\n",
    "\n",
    "            # backward pass\n",
    "            # PyTorch sums up gradients that are computed in sequence\n",
    "            # so unless we \"erase\" those gradients after every update, \n",
    "            # we will backpropagate through different batches\n",
    "            # so we set them to zero every time\n",
    "            optimizer.zero_grad()\n",
    "            # here we compute gradients\n",
    "            loss_value.backward()\n",
    "            # here we update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss_value.item()\n",
    "\n",
    "            # log batch stats    \n",
    "            if print_batch_stats:\n",
    "                print(f\"| Batch {batch_num+1:6d}/{num_batches:6d} \"\n",
    "                      f\"| Loss {loss_value:6.4f} \"\n",
    "                      f\"| Batch PPL {math.exp(loss_value):8.2f}\")\n",
    "        \n",
    "        # compute avg loss per batch\n",
    "        avg_loss = total_loss / num_batches\n",
    "        \n",
    "        # compute perplexity where avg loss is likelihood (empirical risk)\n",
    "        ppl = math.exp(avg_loss)\n",
    "\n",
    "        # compute epoch time\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        # log epoch stats\n",
    "        print(\n",
    "            f\"| Epoch {epoch+1:2d}/{num_epochs:2d} | Epoch Time {epoch_time:5f} \"\n",
    "            f\"| Avg Loss {avg_loss:6.4f} | PPL {ppl:8.2f}\"\n",
    "        )\n",
    "\n",
    "        # reset total loss and timer\n",
    "        total_loss = 0.\n",
    "        start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (linear2): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Embedding(29245, 16)\n",
      "  (decoder): Linear(in_features=16, out_features=29245, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate your transformer\n",
    "num_tokens = len(shakespeare_vocab)\n",
    "token_size = 16 \n",
    "hidden_size = 16\n",
    "num_layers = 2\n",
    "num_heads = 2\n",
    "transformer_lm = TransformerModel(num_tokens,token_size,num_heads,hidden_size,num_layers).to(DEVICE)\n",
    "print(transformer_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch  1/ 1 | Epoch Time 5.123611 | Avg Loss 6.4762 | PPL   649.48\n"
     ]
    }
   ],
   "source": [
    "# train your transformer\n",
    "train(transformer_lm, training_dataloader, num_tokens, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our evaluation loop\n",
    "def evaluate(model, dataloader, num_tokens, print_batch_stats=False):\n",
    "    \"\"\"\n",
    "    Evaluate model on given dataset.\n",
    "\n",
    "    Args:\n",
    "        model: some LM implemented in PyTorch\n",
    "        dataloader: dataloader that returns sentences as examples\n",
    "    \"\"\"\n",
    "\n",
    "    # we use cross entropy so we can compute perplexity from this\n",
    "    # we sum loss up, to then divide by number of examples\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    # set model to eval mode (turns off dropout, etc.)\n",
    "    model.eval()\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "    num_examples = 0\n",
    "    total_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # we iterate over batches\n",
    "        start_time = time.time()\n",
    "        for batch_num, (inputs, targets) in enumerate(dataloader):\n",
    "\n",
    "            # move inputs and targets to device\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "\n",
    "            # add up number of examples\n",
    "            num_examples += len(inputs)\n",
    "\n",
    "            # compute loss\n",
    "            output = model(inputs)\n",
    "            loss_value = loss_fn(output[:, -1, :], targets.view(-1))\n",
    "\n",
    "            # add up loss\n",
    "            total_loss += loss_value.item()\n",
    "\n",
    "            # log batch stats    \n",
    "            if print_batch_stats:\n",
    "                print(f\"| Batch {batch_num+1:6d}/{num_batches:6d} \"\n",
    "                        f\"| Loss {loss_value:6.4f} \"\n",
    "                        f\"| Batch PPL {math.exp(loss_value):8.2f}\")\n",
    "        \n",
    "        # compute avg loss per batch\n",
    "        avg_loss = total_loss / num_examples\n",
    "        \n",
    "        # compute perplexity where avg loss is likelihood (empirical risk)\n",
    "        ppl = math.exp(avg_loss)\n",
    "\n",
    "        # compute epoch time\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        # log epoch stats\n",
    "        print(\n",
    "            f\"| Run Time {total_time:5f} \"\n",
    "            f\"| Avg Loss {avg_loss:6.4f} \"\n",
    "            f\"| PPL {ppl:8.2f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our collate function for validation data\n",
    "# we use the same as with the FNN and RNN for comparable results\n",
    "# this one has a single target per input sequence, unlike the one used for\n",
    "# teacher forcing during training\n",
    "def validation_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Function to construct labeled example from given batch.\n",
    "\n",
    "    Args:\n",
    "        batch (tensor): tensor of size batch_size x sentence_length with tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # we create two lists for our training examples: inputs and corresponding \n",
    "    # targets    \n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for example in batch:\n",
    "        inputs.append(torch.tensor(example[:-1]))\n",
    "        targets.append(torch.tensor(example[-1]))\n",
    "\n",
    "    return torch.stack(inputs, dim=0), torch.stack(targets, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.SelfSupervisedTextDataset object at 0x7fe3c1bd6df0>\n"
     ]
    }
   ],
   "source": [
    "# our validation dataset\n",
    "validation_dataset = SelfSupervisedTextDataset(shakespeare_splits_ids[\"valid\"], \n",
    "                                               max_input_length)\n",
    "print(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our dataloader for validation\n",
    "batch_size = 128 \n",
    "validation_dataloader = DataLoader(validation_dataset, \n",
    "                                 collate_fn=validation_collate_fn,\n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=True, \n",
    "                                 num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Run Time 0.073762 | Avg Loss 6.1747 | PPL   480.45\n"
     ]
    }
   ],
   "source": [
    "# evaluate transformer\n",
    "# we use the same dataloader as with our model model, so performance is \n",
    "# comparable, but the evaluate function must be able to handle both models\n",
    "evaluate(transformer_lm, validation_dataloader, num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPL on Shakespeare's Validation Split\n",
    "\n",
    "* N-Gram: ~16K (different construction of validation examples, i.e. perhaps not comparable)\n",
    "* FNN: ~5K\n",
    "* RNN: ~400\n",
    "* TF1: ~500 (2 layers, 2 attention heads each)\n",
    "\n",
    "Traning and evaluation conditions and are the same for the RNN and TF1, so based on these numbers, RNN wins.\n",
    "But more can and should be done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (linear2): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Embedding(29245, 16)\n",
      "  (decoder): Linear(in_features=16, out_features=29245, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# let's train it for longer to see if it converges\n",
    "num_tokens = len(shakespeare_vocab)\n",
    "token_size = 16 \n",
    "hidden_size = 16\n",
    "num_layers = 2\n",
    "num_heads = 2\n",
    "tf1 = TransformerModel(num_tokens,token_size,num_heads,hidden_size,num_layers).to(DEVICE)\n",
    "print(tf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch  1/20 | Epoch Time 5.098485 | Avg Loss 6.4761 | PPL   649.44\n",
      "| Epoch  2/20 | Epoch Time 5.067635 | Avg Loss 6.0588 | PPL   427.88\n",
      "| Epoch  3/20 | Epoch Time 5.197321 | Avg Loss 5.9272 | PPL   375.09\n",
      "| Epoch  4/20 | Epoch Time 5.199514 | Avg Loss 5.8511 | PPL   347.62\n",
      "| Epoch  5/20 | Epoch Time 5.200850 | Avg Loss 5.8010 | PPL   330.64\n",
      "| Epoch  6/20 | Epoch Time 5.218910 | Avg Loss 5.7631 | PPL   318.34\n",
      "| Epoch  7/20 | Epoch Time 5.123864 | Avg Loss 5.7347 | PPL   309.43\n",
      "| Epoch  8/20 | Epoch Time 5.054551 | Avg Loss 5.7117 | PPL   302.38\n",
      "| Epoch  9/20 | Epoch Time 5.040707 | Avg Loss 5.6938 | PPL   297.01\n",
      "| Epoch 10/20 | Epoch Time 5.039849 | Avg Loss 5.6782 | PPL   292.43\n",
      "| Epoch 11/20 | Epoch Time 5.029094 | Avg Loss 5.6638 | PPL   288.24\n",
      "| Epoch 12/20 | Epoch Time 5.067154 | Avg Loss 5.6527 | PPL   285.05\n",
      "| Epoch 13/20 | Epoch Time 5.078906 | Avg Loss 5.6426 | PPL   282.19\n",
      "| Epoch 14/20 | Epoch Time 5.056858 | Avg Loss 5.6336 | PPL   279.68\n",
      "| Epoch 15/20 | Epoch Time 5.034527 | Avg Loss 5.6255 | PPL   277.40\n",
      "| Epoch 16/20 | Epoch Time 5.037759 | Avg Loss 5.6167 | PPL   274.98\n",
      "| Epoch 17/20 | Epoch Time 5.086126 | Avg Loss 5.6104 | PPL   273.25\n",
      "| Epoch 18/20 | Epoch Time 5.044658 | Avg Loss 5.6046 | PPL   271.67\n",
      "| Epoch 19/20 | Epoch Time 5.046406 | Avg Loss 5.5985 | PPL   270.02\n",
      "| Epoch 20/20 | Epoch Time 5.085421 | Avg Loss 5.5938 | PPL   268.76\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "train(tf1, training_dataloader, num_tokens, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Run Time 0.039761 | Avg Loss 5.9963 | PPL   401.92\n"
     ]
    }
   ],
   "source": [
    "# evaluate \n",
    "evaluate(tf1, validation_dataloader, num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (linear2): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Embedding(29245, 16)\n",
      "  (decoder): Linear(in_features=16, out_features=29245, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# let's try decreasing depth\n",
    "num_tokens = len(shakespeare_vocab)\n",
    "token_size = 16 \n",
    "hidden_size = 16\n",
    "num_layers = 1\n",
    "num_heads = 2\n",
    "tf2 = TransformerModel(num_tokens,token_size,num_heads,hidden_size,num_layers).to(DEVICE)\n",
    "print(tf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch  1/20 | Epoch Time 4.701346 | Avg Loss 6.4598 | PPL   638.92\n",
      "| Epoch  2/20 | Epoch Time 4.888820 | Avg Loss 6.0790 | PPL   436.61\n",
      "| Epoch  3/20 | Epoch Time 4.950058 | Avg Loss 5.9554 | PPL   385.84\n",
      "| Epoch  4/20 | Epoch Time 4.842011 | Avg Loss 5.8824 | PPL   358.66\n",
      "| Epoch  5/20 | Epoch Time 4.785655 | Avg Loss 5.8323 | PPL   341.13\n",
      "| Epoch  6/20 | Epoch Time 4.818069 | Avg Loss 5.7980 | PPL   329.65\n",
      "| Epoch  7/20 | Epoch Time 4.924874 | Avg Loss 5.7694 | PPL   320.33\n",
      "| Epoch  8/20 | Epoch Time 4.792286 | Avg Loss 5.7468 | PPL   313.17\n",
      "| Epoch  9/20 | Epoch Time 4.782013 | Avg Loss 5.7294 | PPL   307.78\n",
      "| Epoch 10/20 | Epoch Time 4.815236 | Avg Loss 5.7138 | PPL   303.01\n",
      "| Epoch 11/20 | Epoch Time 4.833667 | Avg Loss 5.6987 | PPL   298.47\n",
      "| Epoch 12/20 | Epoch Time 4.810474 | Avg Loss 5.6867 | PPL   294.91\n",
      "| Epoch 13/20 | Epoch Time 4.842332 | Avg Loss 5.6763 | PPL   291.87\n",
      "| Epoch 14/20 | Epoch Time 5.184415 | Avg Loss 5.6671 | PPL   289.19\n",
      "| Epoch 15/20 | Epoch Time 5.343128 | Avg Loss 5.6588 | PPL   286.81\n",
      "| Epoch 16/20 | Epoch Time 5.353203 | Avg Loss 5.6513 | PPL   284.66\n",
      "| Epoch 17/20 | Epoch Time 5.414593 | Avg Loss 5.6450 | PPL   282.88\n",
      "| Epoch 18/20 | Epoch Time 5.334795 | Avg Loss 5.6367 | PPL   280.53\n",
      "| Epoch 19/20 | Epoch Time 4.905706 | Avg Loss 5.6304 | PPL   278.77\n",
      "| Epoch 20/20 | Epoch Time 4.908443 | Avg Loss 5.6256 | PPL   277.43\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "train(tf2, training_dataloader, num_tokens, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Run Time 0.036302 | Avg Loss 6.0790 | PPL   436.61\n"
     ]
    }
   ],
   "source": [
    "# evaluate \n",
    "evaluate(tf2, validation_dataloader, num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (linear2): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Embedding(29245, 16)\n",
      "  (decoder): Linear(in_features=16, out_features=29245, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/druffine/anaconda3/envs/hf_practice/lib/python3.9/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# let's try increasing depth\n",
    "num_tokens = len(shakespeare_vocab)\n",
    "token_size = 16 \n",
    "hidden_size = 16\n",
    "num_layers = 3\n",
    "num_heads = 2\n",
    "tf3 = TransformerModel(num_tokens,token_size,num_heads,hidden_size,num_layers).to(DEVICE)\n",
    "print(tf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch  1/20 | Epoch Time 5.400482 | Avg Loss 6.5141 | PPL   674.56\n",
      "| Epoch  2/20 | Epoch Time 5.381609 | Avg Loss 6.0816 | PPL   437.71\n",
      "| Epoch  3/20 | Epoch Time 5.315792 | Avg Loss 5.9446 | PPL   381.68\n",
      "| Epoch  4/20 | Epoch Time 5.308254 | Avg Loss 5.8695 | PPL   354.09\n",
      "| Epoch  5/20 | Epoch Time 5.294880 | Avg Loss 5.8195 | PPL   336.81\n",
      "| Epoch  6/20 | Epoch Time 5.505810 | Avg Loss 5.7831 | PPL   324.75\n",
      "| Epoch  7/20 | Epoch Time 5.493141 | Avg Loss 5.7546 | PPL   315.63\n",
      "| Epoch  8/20 | Epoch Time 5.524028 | Avg Loss 5.7316 | PPL   308.46\n",
      "| Epoch  9/20 | Epoch Time 5.367386 | Avg Loss 5.7119 | PPL   302.45\n",
      "| Epoch 10/20 | Epoch Time 5.316175 | Avg Loss 5.6956 | PPL   297.57\n",
      "| Epoch 11/20 | Epoch Time 5.361366 | Avg Loss 5.6813 | PPL   293.34\n",
      "| Epoch 12/20 | Epoch Time 5.316273 | Avg Loss 5.6677 | PPL   289.38\n",
      "| Epoch 13/20 | Epoch Time 5.427605 | Avg Loss 5.6566 | PPL   286.18\n",
      "| Epoch 14/20 | Epoch Time 5.351125 | Avg Loss 5.6464 | PPL   283.28\n",
      "| Epoch 15/20 | Epoch Time 5.363863 | Avg Loss 5.6375 | PPL   280.77\n",
      "| Epoch 16/20 | Epoch Time 5.332977 | Avg Loss 5.6285 | PPL   278.25\n",
      "| Epoch 17/20 | Epoch Time 5.440694 | Avg Loss 5.6216 | PPL   276.34\n",
      "| Epoch 18/20 | Epoch Time 5.361462 | Avg Loss 5.6162 | PPL   274.83\n",
      "| Epoch 19/20 | Epoch Time 5.361744 | Avg Loss 5.6083 | PPL   272.67\n",
      "| Epoch 20/20 | Epoch Time 5.382666 | Avg Loss 5.6023 | PPL   271.05\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "train(tf3, training_dataloader, num_tokens, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Run Time 0.045038 | Avg Loss 5.9818 | PPL   396.16\n"
     ]
    }
   ],
   "source": [
    "# evaluate \n",
    "evaluate(tf3, validation_dataloader, num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (linear2): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Embedding(29245, 16)\n",
      "  (decoder): Linear(in_features=16, out_features=29245, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/druffine/anaconda3/envs/hf_practice/lib/python3.9/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# let's try increasing depth even more\n",
    "num_tokens = len(shakespeare_vocab)\n",
    "token_size = 16 \n",
    "hidden_size = 16\n",
    "num_layers = 6\n",
    "num_heads = 2\n",
    "tf4 = TransformerModel(num_tokens,token_size,num_heads,hidden_size,num_layers).to(DEVICE)\n",
    "print(tf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch  1/20 | Epoch Time 6.254042 | Avg Loss 6.6811 | PPL   797.21\n",
      "| Epoch  2/20 | Epoch Time 6.256685 | Avg Loss 6.5647 | PPL   709.58\n",
      "| Epoch  3/20 | Epoch Time 6.228310 | Avg Loss 6.5253 | PPL   682.17\n",
      "| Epoch  4/20 | Epoch Time 6.303316 | Avg Loss 6.3468 | PPL   570.67\n",
      "| Epoch  5/20 | Epoch Time 6.451703 | Avg Loss 6.2024 | PPL   493.95\n",
      "| Epoch  6/20 | Epoch Time 6.476623 | Avg Loss 6.1223 | PPL   455.93\n",
      "| Epoch  7/20 | Epoch Time 6.303729 | Avg Loss 6.0588 | PPL   427.84\n",
      "| Epoch  8/20 | Epoch Time 6.258466 | Avg Loss 6.0063 | PPL   405.97\n",
      "| Epoch  9/20 | Epoch Time 6.419229 | Avg Loss 5.9625 | PPL   388.57\n",
      "| Epoch 10/20 | Epoch Time 6.438334 | Avg Loss 5.9225 | PPL   373.36\n",
      "| Epoch 11/20 | Epoch Time 6.477327 | Avg Loss 5.8882 | PPL   360.77\n",
      "| Epoch 12/20 | Epoch Time 6.480778 | Avg Loss 5.8557 | PPL   349.22\n",
      "| Epoch 13/20 | Epoch Time 6.481721 | Avg Loss 5.8287 | PPL   339.90\n",
      "| Epoch 14/20 | Epoch Time 6.456977 | Avg Loss 5.8051 | PPL   331.98\n",
      "| Epoch 15/20 | Epoch Time 6.409601 | Avg Loss 5.7861 | PPL   325.73\n",
      "| Epoch 16/20 | Epoch Time 6.271814 | Avg Loss 5.7684 | PPL   320.02\n",
      "| Epoch 17/20 | Epoch Time 6.685576 | Avg Loss 5.7516 | PPL   314.70\n",
      "| Epoch 18/20 | Epoch Time 6.648481 | Avg Loss 5.7384 | PPL   310.56\n",
      "| Epoch 19/20 | Epoch Time 6.527521 | Avg Loss 5.7244 | PPL   306.24\n",
      "| Epoch 20/20 | Epoch Time 6.288762 | Avg Loss 5.7126 | PPL   302.66\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "train(tf4, training_dataloader, num_tokens, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Run Time 0.056360 | Avg Loss 6.0392 | PPL   419.54\n"
     ]
    }
   ],
   "source": [
    "# evaluate \n",
    "evaluate(tf4, validation_dataloader, num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about training with a learning rate scheduler\n",
    "import time, math\n",
    "\n",
    "def train_with_scheduler(\n",
    "        model, \n",
    "        dataloader, \n",
    "        num_tokens, \n",
    "        num_epochs=10,\n",
    "        lr=1.0,\n",
    "        print_batch_stats=False\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "\n",
    "    Args:\n",
    "        model: some LM implemented in PyTorch\n",
    "        dataloader: dataloader that returns sentences as examples\n",
    "        num_epochs (int): number of epochs to train \n",
    "        lr (float): learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    # set training hyperparameters\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    learning_rate = lr\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # we iterate over epochs\n",
    "    num_batches = len(dataloader)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.\n",
    "        start_time = time.time()\n",
    "\n",
    "        # we iterate over batches\n",
    "        for batch_num, (inputs, targets) in enumerate(dataloader):\n",
    "\n",
    "            # move inputs and targets to device\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "\n",
    "            # get current learning rate\n",
    "            learning_rate = scheduler.get_last_lr()[0]\n",
    "\n",
    "            # forward pass\n",
    "            output = model(inputs)\n",
    "            loss_value = loss_fn(output.view(-1, num_tokens), targets.view(-1))\n",
    "\n",
    "            # backward pass\n",
    "            # PyTorch sums up gradients that are computed in sequence\n",
    "            # so unless we \"erase\" those gradients after every update, \n",
    "            # we will backpropagate through different batches\n",
    "            # so we set them to zero every time\n",
    "            optimizer.zero_grad()\n",
    "            # here we compute gradients\n",
    "            loss_value.backward()\n",
    "            # here we update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss_value.item()\n",
    "\n",
    "            # log batch stats    \n",
    "            if print_batch_stats:\n",
    "                print(f\"| Batch {batch_num+1:6d}/{num_batches:6d} \"\n",
    "                      f\"| LR {learning_rate:6.4f} \"\n",
    "                      f\"| Loss {loss_value:6.4f} \"\n",
    "                      f\"| Batch PPL {math.exp(loss_value):8.2f}\")\n",
    "        \n",
    "        # compute avg loss per batch\n",
    "        avg_loss = total_loss / num_batches\n",
    "        \n",
    "        # compute perplexity where avg loss is likelihood (empirical risk)\n",
    "        ppl = math.exp(avg_loss)\n",
    "\n",
    "        # compute epoch time\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        # log epoch stats\n",
    "        print(\n",
    "            f\"| Epoch {epoch+1:2d}/{num_epochs:2d} | Epoch Time {epoch_time:5f} \"\n",
    "            f\"| Avg Loss {avg_loss:6.4f} | PPL {ppl:8.2f}\"\n",
    "        )\n",
    "\n",
    "        # reset total loss and timer\n",
    "        total_loss = 0.\n",
    "        start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (linear2): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Embedding(29245, 16)\n",
      "  (decoder): Linear(in_features=16, out_features=29245, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#  we'll train this one with a learning rate scheduler\n",
    "m_tokens = len(shakespeare_vocab)\n",
    "token_size = 16 \n",
    "hidden_size = 16\n",
    "num_layers = 3\n",
    "num_heads = 2\n",
    "lr = 0.1\n",
    "tf5 = TransformerModel(num_tokens,token_size,num_heads,hidden_size,num_layers).to(DEVICE)\n",
    "print(tf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch  1/20 | Epoch Time 5.361673 | Avg Loss 6.5346 | PPL   688.59\n",
      "| Epoch  2/20 | Epoch Time 5.328326 | Avg Loss 6.1103 | PPL   450.46\n",
      "| Epoch  3/20 | Epoch Time 5.340834 | Avg Loss 5.9639 | PPL   389.13\n",
      "| Epoch  4/20 | Epoch Time 5.381404 | Avg Loss 5.8837 | PPL   359.13\n",
      "| Epoch  5/20 | Epoch Time 5.356963 | Avg Loss 5.8315 | PPL   340.88\n",
      "| Epoch  6/20 | Epoch Time 5.362206 | Avg Loss 5.7950 | PPL   328.67\n",
      "| Epoch  7/20 | Epoch Time 5.439713 | Avg Loss 5.7629 | PPL   318.27\n",
      "| Epoch  8/20 | Epoch Time 5.450390 | Avg Loss 5.7386 | PPL   310.64\n",
      "| Epoch  9/20 | Epoch Time 5.351116 | Avg Loss 5.7202 | PPL   304.97\n",
      "| Epoch 10/20 | Epoch Time 5.383507 | Avg Loss 5.7011 | PPL   299.20\n",
      "| Epoch 11/20 | Epoch Time 5.360246 | Avg Loss 5.6869 | PPL   294.97\n",
      "| Epoch 12/20 | Epoch Time 5.376559 | Avg Loss 5.6735 | PPL   291.06\n",
      "| Epoch 13/20 | Epoch Time 5.373191 | Avg Loss 5.6632 | PPL   288.07\n",
      "| Epoch 14/20 | Epoch Time 5.380653 | Avg Loss 5.6522 | PPL   284.93\n",
      "| Epoch 15/20 | Epoch Time 5.354458 | Avg Loss 5.6441 | PPL   282.61\n",
      "| Epoch 16/20 | Epoch Time 5.365797 | Avg Loss 5.6345 | PPL   279.92\n",
      "| Epoch 17/20 | Epoch Time 5.446447 | Avg Loss 5.6267 | PPL   277.75\n",
      "| Epoch 18/20 | Epoch Time 5.370296 | Avg Loss 5.6200 | PPL   275.88\n",
      "| Epoch 19/20 | Epoch Time 5.354933 | Avg Loss 5.6134 | PPL   274.07\n",
      "| Epoch 20/20 | Epoch Time 5.361385 | Avg Loss 5.6075 | PPL   272.47\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "train_with_scheduler(tf5, training_dataloader, num_tokens, num_epochs=20, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Run Time 0.045606 | Avg Loss 5.9262 | PPL   374.72\n"
     ]
    }
   ],
   "source": [
    "# evaluate \n",
    "evaluate(tf5, validation_dataloader, num_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
