{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Methods in Text Analytics\n",
    "# Exercise 4: Language Models - Part 2\n",
    "### Daniel Ruffinelli\n",
    "## FSS 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. N-Gram Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a simple whitespace tokenizer that removes punctuation\n",
    "def tokenize(text):\n",
    "    \"\"\" \n",
    "    Given text, returns all words separated by white space after separating all\n",
    "    punctuation.\n",
    "\n",
    "    Args:\n",
    "        text: string with text to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list of tokens\n",
    "    \"\"\"\n",
    "\n",
    "    import string\n",
    "\n",
    "    # separate punctuation symbols with whitespaces\n",
    "    for symbol in string.punctuation:\n",
    "        text = text.replace(symbol, \" \" + symbol + \" \")\n",
    "    text_split = text.split()\n",
    "\n",
    "    return text_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'phrase', ',', 'with', 'some', 'punctuation', ',', 'that', 'we', 'want', 'to', 'tokenize', '!']\n"
     ]
    }
   ],
   "source": [
    "# we test the tokenizer\n",
    "text = \"This is a phrase, with some punctuation, that we want to tokenize!\"\n",
    "print(tokenize(text))\n",
    "# output should be:\n",
    "# ['This', 'is', 'a', 'phrase', ',', 'with', 'some', 'punctuation', ',', 'that', 'we', 'want', 'to', 'tokenize', '!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for counting n-grams\n",
    "def compute_ngrams(n, tokenized_text):\n",
    "    \"\"\" \n",
    "    Compute n-grams in given list of tokens.\n",
    "\n",
    "    Args:\n",
    "        n: size of n-grams\n",
    "        tokens: list of tokens\n",
    "\n",
    "    Returns\n",
    "        list of n-grams of the form (context, next_word), where context is a \n",
    "        tuple of n -1 previous words.\n",
    "    \"\"\"\n",
    "\n",
    "    # we add left-side padding (i.e. start of sentence symbols)\n",
    "    tokens = (n-1)*[\"<s>\"] + tokenized_text\n",
    "\n",
    "    # list to store ngrams\n",
    "    ngrams = []\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    # iterate over sequence of tokens\n",
    "    for i in range(n-1, len(tokens)):\n",
    "        # iterate over context (n-1 previous words for current token)\n",
    "        context = []\n",
    "        for j in reversed(range(n-1)):\n",
    "            context.append((tokens[i-j-1]))\n",
    "        ngrams.append((tuple(context), tokens[i]))\n",
    "\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('<s>', '<s>'), 'This'), (('<s>', 'This'), 'is'), (('This', 'is'), 'a'), (('is', 'a'), 'phrase'), (('a', 'phrase'), ','), (('phrase', ','), 'with'), ((',', 'with'), 'some'), (('with', 'some'), 'punctuation'), (('some', 'punctuation'), ','), (('punctuation', ','), 'that'), ((',', 'that'), 'we'), (('that', 'we'), 'want'), (('we', 'want'), 'to'), (('want', 'to'), 'tokenize'), (('to', 'tokenize'), '!')]\n"
     ]
    }
   ],
   "source": [
    "# test your ngram computation\n",
    "text = \"This is a phrase, with some punctuation, that we want to tokenize!\"\n",
    "print(compute_ngrams(3, tokenize(text)))\n",
    "# output should be:\n",
    "# [(('<s>', '<s>'), 'This'), (('<s>', 'This'), 'is'), (('This', 'is'), 'a'), (('is', 'a'), 'phrase'), (('a', 'phrase'), ','), (('phrase', ','), 'with'), ((',', 'with'), 'some'), (('with', 'some'), 'punctuation'), (('some', 'punctuation'), ','), (('punctuation', ','), 'that'), ((',', 'that'), 'we'), (('that', 'we'), 'want'), (('we', 'want'), 'to'), (('want', 'to'), 'tokenize'), (('to', 'tokenize'), '!')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an n-gram model that keeps track of \"context to next word\" pairs\n",
    "# and an ngram counter\n",
    "from collections import defaultdict as ddict \n",
    "\n",
    "class NGramModel:\n",
    "    \"\"\"\n",
    "    NGramModel class, keeps track of (context, next_word) pairs and an ngram\n",
    "    counter.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "\n",
    "        self._n = n\n",
    "\n",
    "        # set to store vocabulary\n",
    "        self.vocabulary = set()\n",
    "\n",
    "        # dict to store ngram counters\n",
    "        self.ngram_counter = ddict(int)\n",
    "\n",
    "        # dict to track (context, next_word) pairs\n",
    "        self.context = ddict(list)\n",
    "\n",
    "    def extend_vocabulary(self, tokenized_text):\n",
    "        \"\"\" \n",
    "        Adds tokens to vocabulary. Useful for smoothing for unseen data.\n",
    "        \"\"\"\n",
    "        for token in tokenized_text:\n",
    "            self.vocabulary.add(token)\n",
    "\n",
    "    def update(self, tokenized_text):\n",
    "        \"\"\" \n",
    "        Updates counts of ngram model by computing ngrams in given tokenized \n",
    "        text and adding them to ngram counter and context tracker.\n",
    "\n",
    "        Args:\n",
    "            text (str): text to tokenize and compute ngrams\n",
    "        \"\"\"\n",
    "\n",
    "        # compute ngrams\n",
    "        ngrams = compute_ngrams(self._n, tokenized_text)\n",
    "\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "        # update vocabulary\n",
    "        self.extend_vocabulary(tokenized_text)\n",
    "\n",
    "        # update model parameters\n",
    "        for ngram in ngrams:\n",
    "            # update ngram counts\n",
    "            self.ngram_counter[ngram] += 1\n",
    "\n",
    "            # update context tracking\n",
    "            context, next_word = ngram\n",
    "            self.context[context].append(next_word)\n",
    "\n",
    "    def next_word_prob(self, context, next_word):\n",
    "        \"\"\"\n",
    "        Returns probability of predicting next_word after given context.\n",
    "        We use add-1 smoothing to process unseen words.\n",
    "\n",
    "        Args:\n",
    "            context (list of tokens): context words\n",
    "            next_word (str): next word\n",
    "\n",
    "        Returns:\n",
    "            prob (float): probability of next_word following context.\n",
    "        \"\"\"\n",
    "\n",
    "        # probability value to return\n",
    "        prob = 0.\n",
    "\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "        try:\n",
    "            # number of times we saw next_word follow context\n",
    "            numerator = self.ngram_counter[(context, next_word)] + 1\n",
    "        except KeyError:\n",
    "            numerator = 1\n",
    "\n",
    "        # number of times we saw context\n",
    "        denominator = len(self.context[context]) + len(self.vocabulary)\n",
    "\n",
    "        # compute prob\n",
    "        prob = numerator/denominator\n",
    "\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size on n-gram: 2\n"
     ]
    }
   ],
   "source": [
    "# test your NGram model\n",
    "bigram_model = NGramModel(2)\n",
    "print(\"Size on n-gram:\", bigram_model._n)\n",
    "# output should be: Size on n-gram: 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'!', 'bigram', 'our', 'model', 'text', 'add', 'some', 'to', 's', 'now', 'Let', 'We', 'need', \"'\"}\n",
      "N-Gram Counter: defaultdict(<class 'int'>, {(('<s>',), 'We'): 1, (('We',), 'now'): 1, (('now',), 'need'): 1, (('need',), 'text'): 1, (('text',), '!'): 1, (('!',), 'Let'): 1, (('Let',), \"'\"): 1, ((\"'\",), 's'): 1, (('s',), 'add'): 1, (('add',), 'some'): 1, (('some',), 'text'): 1, (('text',), 'to'): 1, (('to',), 'our'): 1, (('our',), 'bigram'): 1, (('bigram',), 'model'): 1})\n",
      "Context Dict: defaultdict(<class 'list'>, {('<s>',): ['We'], ('We',): ['now'], ('now',): ['need'], ('need',): ['text'], ('text',): ['!', 'to'], ('!',): ['Let'], ('Let',): [\"'\"], (\"'\",): ['s'], ('s',): ['add'], ('add',): ['some'], ('some',): ['text'], ('to',): ['our'], ('our',): ['bigram'], ('bigram',): ['model']})\n"
     ]
    }
   ],
   "source": [
    "# test your update function\n",
    "bigram_model.update(tokenize(\"We now need text! Let's add some text to our bigram model\"))\n",
    "print(\"Vocabulary:\", bigram_model.vocabulary)\n",
    "print(\"N-Gram Counter:\", bigram_model.ngram_counter)\n",
    "print(\"Context Dict:\", bigram_model.context)\n",
    "# output should be:\n",
    "# Vocabulary: {'s', '!', 'model', 'to', 'Let', 'add', \"'\", 'bigram', 'need', 'now', 'We', 'text', 'our', 'some'}\n",
    "# N-Gram Counter: defaultdict(<class 'int'>, {(('<s>',), 'We'): 1, (('We',), 'now'): 1, (('now',), 'need'): 1, (('need',), 'text'): 1, (('text',), '!'): 1, (('!',), 'Let'): 1, (('Let',), \"'\"): 1, ((\"'\",), 's'): 1, (('s',), 'add'): 1, (('add',), 'some'): 1, (('some',), 'text'): 1, (('text',), 'to'): 1, (('to',), 'our'): 1, (('our',), 'bigram'): 1, (('bigram',), 'model'): 1})\n",
    "# Context Dict: defaultdict(<class 'list'>, {('<s>',): ['We'], ('We',): ['now'], ('now',): ['need'], ('need',): ['text'], ('text',): ['!', 'to'], ('!',): ['Let'], ('Let',): [\"'\"], (\"'\",): ['s'], ('s',): ['add'], ('add',): ['some'], ('some',): ['text'], ('to',): ['our'], ('our',): ['bigram'], ('bigram',): ['model']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that word 'to' follows word 'text': 0.125\n"
     ]
    }
   ],
   "source": [
    "# test your next_word_prob function\n",
    "print(\"Probability that word 'to' follows word 'text':\", bigram_model.next_word_prob((\"text\",), \"to\"))\n",
    "# output should be:\n",
    "# Probability that word 'to' follows word 'text': 0.125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to predict next word given context\n",
    "def predict_next_word(context, model):\n",
    "    \"\"\"\n",
    "    Uses given model to predict the next word that could follow the given \n",
    "    context. Each possible next word is chosen based on its probability given\n",
    "    by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    import random\n",
    "\n",
    "    # get all possible next words\n",
    "    next_words = model.context[context]\n",
    "    next_words_probs = []\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    # get probabilities of each next_word\n",
    "    for next_word in next_words:\n",
    "        next_words_probs.append(model.next_word_prob(context, next_word))\n",
    "    # normalize probs\n",
    "    prob_mass = 0\n",
    "    for prob in next_words_probs:\n",
    "        prob_mass += prob\n",
    "    for i, prob in enumerate(next_words_probs):\n",
    "        next_words_probs[i] = next_words_probs[i] / prob_mass\n",
    "\n",
    "    # select one of possible next word to predict\n",
    "    predicted_word = random.choices(next_words, next_words_probs, k=1)\n",
    "\n",
    "    return predicted_word[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your function predict_next_word\n",
    "predict_next_word((\"text\",), bigram_model)\n",
    "# output should be either '!' or 'to'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load shakespeare's entire works\n",
    "with open(\"shakespeare_train.txt\") as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "# break it into sentences\n",
    "corpus_sentences = corpus.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a trigram model\n",
    "trigram_model = NGramModel(3)\n",
    "\n",
    "# feed corpus to model\n",
    "for sentence in corpus_sentences:\n",
    "        trigram_model.update(tokenize(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((('<s>', '<s>'), 'I'), 4293),\n",
       " ((('I', \"'\"), 'll'), 1634),\n",
       " ((('<s>', '<s>'), 'The'), 1547),\n",
       " ((('<s>', '<s>'), 'What'), 1510),\n",
       " ((('<s>', '<s>'), '['), 1500),\n",
       " ((('<s>', '<s>'), 'O'), 1342),\n",
       " ((('<s>', '<s>'), \"'\"), 1214),\n",
       " ((('<s>', '<s>'), 'And'), 1087),\n",
       " ((('<s>', '<s>'), 'But'), 1041),\n",
       " ((('<s>', '<s>'), 'Enter'), 1018),\n",
       " (((\"'\", 'd'), ','), 1002),\n",
       " ((('<s>', '<s>'), 'You'), 923),\n",
       " (((',', 'my'), 'lord'), 878),\n",
       " ((('<s>', '<s>'), 'My'), 873),\n",
       " ((('<s>', '<s>'), 'If'), 869),\n",
       " ((('<s>', '<s>'), 'He'), 809),\n",
       " ((('<s>', '<s>'), 'A'), 807),\n",
       " ((('<s>', '<s>'), 'Exeunt'), 806),\n",
       " ((('<s>', '<s>'), 'Why'), 778),\n",
       " ((('<s>', '<s>'), 'KING'), 765),\n",
       " (((',', 'sir'), ','), 757),\n",
       " ((('<s>', '<s>'), 'Exit'), 688),\n",
       " ((('<s>', '<s>'), 'No'), 665),\n",
       " ((('<s>', '<s>'), 'Come'), 662),\n",
       " ((('<s>', 'O'), ','), 640),\n",
       " ((('<s>', '<s>'), 'This'), 625),\n",
       " ((('<s>', '<s>'), 'How'), 610),\n",
       " ((('<s>', '<s>'), 'Ay'), 559),\n",
       " ((('<s>', '<s>'), 'Now'), 556),\n",
       " ((('<s>', '<s>'), 'It'), 550),\n",
       " ((('<s>', 'Why'), ','), 542),\n",
       " ((('<s>', '<s>'), 'Let'), 515),\n",
       " (((\"'\", 'th'), \"'\"), 512),\n",
       " ((('<s>', 'Ay'), ','), 511),\n",
       " (((',', 'I'), \"'\"), 485),\n",
       " ((('<s>', '<s>'), 'So'), 480),\n",
       " ((('<s>', 'Come'), ','), 474),\n",
       " ((('<s>', '<s>'), 'Well'), 470),\n",
       " ((('<s>', 'I'), \"'\"), 461),\n",
       " ((('<s>', 'I'), 'am'), 450),\n",
       " ((('<s>', '<s>'), 'Thou'), 446),\n",
       " ((('<s>', '<s>'), 'That'), 444),\n",
       " ((('my', 'lord'), ','), 443),\n",
       " ((('<s>', '<s>'), 'Then'), 434),\n",
       " ((('<s>', '<s>'), 'We'), 432),\n",
       " ((('<s>', '<s>'), 'To'), 421),\n",
       " ((('<s>', '<s>'), 'Good'), 418),\n",
       " ((('<s>', '<s>'), 'Here'), 417),\n",
       " ((('<s>', '<s>'), 'There'), 397),\n",
       " ((('<s>', '<s>'), 'Nay'), 397),\n",
       " ((('<s>', \"'\"), 'Tis'), 396),\n",
       " ((('<s>', '<s>'), 'Go'), 392),\n",
       " ((('<s>', 'Nay'), ','), 384),\n",
       " ((('<s>', 'No'), ','), 383),\n",
       " ((('<s>', 'I'), 'will'), 378),\n",
       " (((',', 'I'), 'will'), 360),\n",
       " ((('<s>', 'Well'), ','), 355),\n",
       " ((('<s>', '<s>'), 'For'), 348),\n",
       " (((\"'\", 'd'), 'to'), 345),\n",
       " ((('<s>', '<s>'), 'FIRST'), 333),\n",
       " ((('<s>', '<s>'), 'By'), 333),\n",
       " ((('<s>', 'I'), 'have'), 330),\n",
       " ((('<s>', '<s>'), 'Sir'), 330),\n",
       " (((\"'\", 'd'), 'with'), 326),\n",
       " (((',', 'I'), 'am'), 307),\n",
       " ((('<s>', '<s>'), 'Is'), 295),\n",
       " ((('<s>', 'What'), ','), 292),\n",
       " ((('<s>', '<s>'), 'Your'), 284),\n",
       " ((('<s>', '<s>'), 'SECOND'), 284),\n",
       " ((('<s>', '['), 'Aside'), 284),\n",
       " (((\"'\", 'd'), 'in'), 271),\n",
       " ((('<s>', '<s>'), 'In'), 266),\n",
       " (((\"'\", 'd'), ';'), 266),\n",
       " ((('<s>', '<s>'), 'Do'), 264),\n",
       " (((\"'\", 's'), 'a'), 260),\n",
       " (((',', 'and'), 'the'), 259),\n",
       " ((('<s>', '<s>'), 'Ham'), 259),\n",
       " ((('<s>', '<s>'), 'Who'), 258),\n",
       " ((('<s>', 'It'), 'is'), 258),\n",
       " (((',', 'and'), 'I'), 257),\n",
       " ((('<s>', '<s>'), 'DUKE'), 254),\n",
       " (((\"'\", 'd'), 'the'), 253),\n",
       " ((('<s>', 'Now'), ','), 250),\n",
       " ((('<s>', '<s>'), 'Where'), 247),\n",
       " ((('<s>', '<s>'), 'They'), 242),\n",
       " ((('<s>', 'My'), 'lord'), 241),\n",
       " ((('<s>', '<s>'), 'Not'), 241),\n",
       " ((('[', 'Aside'), ']'), 238),\n",
       " ((('<s>', 'Sir'), ','), 237),\n",
       " ((('<s>', '<s>'), 'She'), 236),\n",
       " (((\"'\", 's'), 'the'), 229),\n",
       " ((('i', \"'\"), 'th'), 229),\n",
       " ((('o', \"'\"), 'th'), 229),\n",
       " ((('Re', '-'), 'enter'), 225),\n",
       " ((('father', \"'\"), 's'), 224),\n",
       " (((\"'\", 'd'), 'me'), 224),\n",
       " ((('<s>', '<s>'), 'QUEEN'), 224),\n",
       " ((('that', \"'\"), 's'), 223),\n",
       " (((\"'\", 'st'), 'thou'), 221),\n",
       " (((',', 'I'), 'have'), 221)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check most common ngrams\n",
    "top = 100\n",
    "sorted(trigram_model.ngram_counter.items(), key=lambda x:x[1], reverse=True)[:top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define function to generate n tokens given starting token\n",
    "def generate_text(num_tokens, context, model):\n",
    "    \"\"\"\n",
    "    Generate num_tokens words using given model\n",
    "\n",
    "    Args:\n",
    "        num_tokens (int): number of tokens to generate\n",
    "        context (tuple of previous words): starting prompt\n",
    "        model (NGramModel): ngram model\n",
    "    \"\"\"\n",
    "\n",
    "    output = []\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    for word in context:\n",
    "        output.append(word)\n",
    "    for i in range(num_tokens):\n",
    "        output.append(predict_next_word(context, model))\n",
    "        context = tuple(list(context[1:]) + [output[-1]])\n",
    "\n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My lord , I ' ll speak with her , and that ' s eyes , And in the advantage of the city , and I ' ll give thee all my heart , to the King , Nor thou with thy\n"
     ]
    }
   ],
   "source": [
    "# predict next words\n",
    "context = (\"My\", \"lord\")\n",
    "\n",
    "# test text generation\n",
    "# this may fail at times because our simple model does not have an entry for the\n",
    "# given context in its {context: next_word} dict, which in turn happens because\n",
    "# the generated context (prompt) is a combination the model has not seen\n",
    "# In short, it's a problem of our implementation that uses a dict\n",
    "# In practice, it's not often done like this (we'll see in the next tasks)\n",
    "print(generate_text(40, context, trigram_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((('<s>', '<s>', '<s>'), 'I'), 4293),\n",
       " ((('<s>', '<s>', '<s>'), 'The'), 1547),\n",
       " ((('<s>', '<s>', '<s>'), 'What'), 1510),\n",
       " ((('<s>', '<s>', '<s>'), '['), 1500),\n",
       " ((('<s>', '<s>', '<s>'), 'O'), 1342),\n",
       " ((('<s>', '<s>', '<s>'), \"'\"), 1214),\n",
       " ((('<s>', '<s>', '<s>'), 'And'), 1087),\n",
       " ((('<s>', '<s>', '<s>'), 'But'), 1041),\n",
       " ((('<s>', '<s>', '<s>'), 'Enter'), 1018),\n",
       " ((('<s>', '<s>', '<s>'), 'You'), 923)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try stronger models\n",
    "fourgram_model = NGramModel(4)\n",
    "\n",
    "# feed corpus to model\n",
    "# fourgram_model.update(tokenize(corpus))\n",
    "for sentence in corpus_sentences:\n",
    "        fourgram_model.update(tokenize(sentence))\n",
    "\n",
    "# check most common ngrams\n",
    "top = 10\n",
    "sorted(fourgram_model.ngram_counter.items(), key=lambda x:x[1], reverse=True)[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ' ll be with you , sir , to speak more properly , stays me at home ; For by this knot\n"
     ]
    }
   ],
   "source": [
    "# predict next words\n",
    "context = (\"I\", \"'\", \"ll\")\n",
    "\n",
    "# test text generation\n",
    "print(generate_text(20, context, fourgram_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((('<s>', '<s>', '<s>', '<s>'), 'I'), 4293),\n",
       " ((('<s>', '<s>', '<s>', '<s>'), 'The'), 1547),\n",
       " ((('<s>', '<s>', '<s>', '<s>'), 'What'), 1510),\n",
       " ((('<s>', '<s>', '<s>', '<s>'), '['), 1500),\n",
       " ((('<s>', '<s>', '<s>', '<s>'), 'O'), 1342),\n",
       " ((('<s>', '<s>', '<s>', '<s>'), \"'\"), 1214),\n",
       " ((('<s>', '<s>', '<s>', '<s>'), 'And'), 1087),\n",
       " ((('<s>', '<s>', '<s>', '<s>'), 'But'), 1041),\n",
       " ((('<s>', '<s>', '<s>', '<s>'), 'Enter'), 1018),\n",
       " ((('<s>', '<s>', '<s>', '<s>'), 'You'), 923)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and a 5-gram models\n",
    "fivegram_model = NGramModel(5)\n",
    "\n",
    "# feed corpus to model\n",
    "# fivegram_model.update(tokenize(corpus))\n",
    "for sentence in corpus_sentences:\n",
    "        fivegram_model.update(tokenize(sentence))\n",
    "\n",
    "# check most common ngrams\n",
    "top = 10\n",
    "sorted(fivegram_model.ngram_counter.items(), key=lambda x:x[1], reverse=True)[:top]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> <s> <s> I ' ll give you a pottle of burnt sack to give me recourse to him , and will make\n"
     ]
    }
   ],
   "source": [
    "# predict next words\n",
    "context = (\"<s>\", \"<s>\", \"<s>\", \"<s>\")\n",
    "\n",
    "# test text generation\n",
    "print(generate_text(20, context, fivegram_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute likelihood of validation data\n",
    "def compute_likelihood(tokenized_text, model, n):\n",
    "    \"\"\"\n",
    "    Compute log_likelihood of given tokenized text using given model.\n",
    "    We compute likelihood in log space to prevent numerical underflow.\n",
    "\n",
    "    Args:\n",
    "        tokenized_text (list of tokens): text to compute likelihood of\n",
    "        model (NGramModel): model used to compute likelihood\n",
    "        n (int): size of n-grams of givem model\n",
    "    \"\"\"\n",
    "\n",
    "    import math\n",
    "\n",
    "    log_likelihood = 0.0\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    for i, token in enumerate(tokenized_text[n:], start=n):\n",
    "        context = tuple(tokenized_text[i-n+1:i])\n",
    "        log_likelihood += math.log(model.next_word_prob(context, token))\n",
    "\n",
    "    return log_likelihood, math.exp(-log_likelihood / i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1605', 'THE', 'TRAGEDY', 'OF', 'OTHELLO', ',', 'MOOR', 'OF', 'VENICE', 'by', 'William', 'Shakespeare', 'Dramatis', 'Personae', 'OTHELLO', ',', 'the', 'Moor', ',', 'general', 'of', 'the', 'Venetian', 'forces', 'DESDEMONA', ',', 'his', 'wife', 'IAGO', ',', 'ensign', 'to', 'Othello', 'EMILIA', ',', 'his', 'wife', ',', 'lady', '-', 'in', '-', 'waiting', 'to', 'Desdemona', 'CASSIO', ',', 'lieutenant', 'to', 'Othello', 'THE', 'DUKE', 'OF', 'VENICE', 'BRABANTIO', ',', 'Venetian', 'Senator', ',', 'father', 'of', 'Desdemona', 'GRATIANO', ',', 'nobleman', 'of', 'Venice', ',', 'brother', 'of', 'Brabantio', 'LODOVICO', ',', 'nobleman', 'of', 'Venice', ',', 'kinsman', 'of', 'Brabantio', 'RODERIGO', ',', 'rejected', 'suitor', 'of', 'Desdemona', 'BIANCA', ',', 'mistress', 'of', 'Cassio', 'MONTANO', ',', 'a', 'Cypriot', 'official', 'A', 'Clown', 'in', 'service']\n"
     ]
    }
   ],
   "source": [
    "# load shakespeare's validation data\n",
    "with open(\"shakespeare_valid.txt\") as f:\n",
    "    corpus_valid = f.read()\n",
    "\n",
    "# tokenize it\n",
    "tokenized_validation = tokenize(corpus_valid)\n",
    "\n",
    "print(tokenized_validation[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation likelihood: -354329.59077216004\n",
      "Validation perplexity: 16012.682762755616\n"
     ]
    }
   ],
   "source": [
    "# extend trigram model vocabulary for proper smoothing\n",
    "trigram_model.extend_vocabulary(tokenized_validation)\n",
    "\n",
    "#  compute validation likelihood\n",
    "valid_likelihood, perplexity = compute_likelihood(tokenize(corpus_valid), trigram_model, 3)\n",
    "print(\"Validation likelihood:\", valid_likelihood)\n",
    "print(\"Validation perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 28858\n"
     ]
    }
   ],
   "source": [
    "# vocabulary size\n",
    "print(\"Vocabulary size:\", len(trigram_model.vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Language Models with Fully-Connected Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7885, 0.5821, 0.4990],\n",
      "        [0.9641, 0.7604, 0.6491],\n",
      "        [0.1664, 0.5281, 0.4575]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# set device to \"cpu\" if you don't have a GPU\n",
    "DEVICE=\"cpu\"\n",
    "\n",
    "# test it\n",
    "a = torch.rand(3,3).to(DEVICE)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 29245\n"
     ]
    }
   ],
   "source": [
    "# let's preprocess our text (we work with embeddings now, not just strings)\n",
    "from collections import defaultdict as ddict\n",
    "\n",
    "# these are our splits\n",
    "shakespeare_splits = {\n",
    "    \"train\": \"shakespeare_train.txt\", \n",
    "    \"valid\": \"shakespeare_valid.txt\", \n",
    "    \"text\": \"shakespeare_test.txt\"\n",
    "}\n",
    "\n",
    "# we create a vocabulary dict of the form {token: ID}\n",
    "shakespeare_vocab = {}\n",
    "for text_file in shakespeare_splits.values():\n",
    "    with open(text_file) as f:\n",
    "        split_text = f.read()\n",
    "        tokenized_split = tokenize(split_text)\n",
    "        for token in tokenized_split:\n",
    "            if token not in shakespeare_vocab:\n",
    "                shakespeare_vocab[token] = len(shakespeare_vocab)\n",
    "# we add the padding symbol to our vocabulary\n",
    "shakespeare_vocab[\"<s>\"] = len(shakespeare_vocab)\n",
    "print(\"Size of vocabulary:\", len(shakespeare_vocab))\n",
    "\n",
    "# we turn our splits into sequences of token IDs\n",
    "shakespeare_splits_ids = ddict(list)\n",
    "for split_id, split_file in shakespeare_splits.items():\n",
    "    with open(split_file) as f:\n",
    "            tokenized_split = tokenize(f.read())\n",
    "    for token in tokenized_split:\n",
    "        shakespeare_splits_ids[split_id].append(shakespeare_vocab[token])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralLM(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 vocabulary_size, \n",
    "                 embedding_size,\n",
    "                 max_input_length,\n",
    "                 padding_token_id,\n",
    "                 hidden_layer_sizes,\n",
    "                 hidden_layer_activation=\"tanh\"):\n",
    "        \"\"\"\n",
    "        Neural language model.\n",
    "\n",
    "        Args:\n",
    "            vocabulary_size (int): size of vocabulary\n",
    "            embedding size (int): size of input tokens\n",
    "            max_input_length (int): max. number of input tokens\n",
    "            padding_token_id (int): id of token to use for left padding\n",
    "            hidden_layer_sizes (list): list of hidden layer sizes, e.g. [10, 5]\n",
    "            hidden_layer_activation (string): activation, e.g. sigmoid, tanh\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # set hyperparameters\n",
    "        self._max_input_length = max_input_length\n",
    "        self._pad_token_id = torch.tensor(padding_token_id).to(DEVICE)\n",
    "        if hasattr(torch, hidden_layer_activation):\n",
    "            self._activation = getattr(torch, hidden_layer_activation)\n",
    "        else:\n",
    "            raise ValueError(\"Activation must be a torch-supported function.\")\n",
    "\n",
    "        # create embedding matrix\n",
    "        self._embeddings = nn.Embedding(vocabulary_size, embedding_size)\n",
    "\n",
    "        # create hidden layers    \n",
    "        hidden_layers = []\n",
    "        input_size = embedding_size * max_input_length\n",
    "        for output_size in hidden_layer_sizes:\n",
    "            hidden_layers.append(\n",
    "                nn.Linear(input_size, output_size).to(DEVICE)\n",
    "                )\n",
    "            input_size = output_size\n",
    "        # see why we require a ModuleList: \n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html\n",
    "        self._hidden_layers = nn.ModuleList(hidden_layers)\n",
    "        \n",
    "        # create output layer (no need for softmax, we'll do that with the loss)\n",
    "        self._output_layer = nn.Linear(output_size, vocabulary_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            seq_indices (tensor): tensor of token IDs of size \n",
    "            (batch_size, max_input_length)\n",
    "\n",
    "        Return:\n",
    "            out (tensor): tensor of size [len(input_ids), vocabulary_size]\n",
    "        \"\"\"\n",
    "\n",
    "        # pad to the left\n",
    "        num_pads = self._max_input_length - input_ids.size()[1]\n",
    "        padding_tensor = self._pad_token_id.expand(input_ids.size()[0])\n",
    "        padding_tensor = torch.unsqueeze(padding_tensor, 1).expand(-1, num_pads)\n",
    "        input_ids = torch.cat(\n",
    "            [padding_tensor, input_ids],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "        # embed input sequence  \n",
    "        # input_ids should be batch_size * max_input_length, but we need to\n",
    "        # get the embeddings for each token in every sequence in the batch.\n",
    "        # So, we first flatten the input_ids, use the flattened indices to get\n",
    "        # the corresponding embeddings for every word in every example in the \n",
    "        # batch, then reshape back to batch_size * (emb_size * input_length)\n",
    "        embs = self._embeddings(\n",
    "                input_ids.view(-1)\n",
    "            ).view(input_ids.size()[0], -1)\n",
    "        out = embs\n",
    "        # compute hidden layers\n",
    "        for hidden_layer in self._hidden_layers:\n",
    "            out = hidden_layer(out)\n",
    "            out = self._activation(out)\n",
    "        # compute output layer\n",
    "        out = self._output_layer(out)\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralLM(\n",
      "  (_embeddings): Embedding(29245, 16)\n",
      "  (_hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=1024, out_features=32, bias=True)\n",
      "  )\n",
      "  (_output_layer): Linear(in_features=32, out_features=29245, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# test your NeuralLM\n",
    "embedding_size = 16\n",
    "hidden_layer_size = 32\n",
    "max_input_length = 64\n",
    "neural_lm_1 = NeuralLM(\n",
    "    len(shakespeare_vocab),\n",
    "    embedding_size=embedding_size,\n",
    "    max_input_length=max_input_length,\n",
    "    padding_token_id=shakespeare_vocab[\"<s>\"],\n",
    "    hidden_layer_sizes=[hidden_layer_size]\n",
    ").to(DEVICE)\n",
    "print(neural_lm_1)\n",
    "\n",
    "# output should be:\n",
    "# NeuralLM(\n",
    "#   (_embeddings): Embedding(29245, 16)\n",
    "#   (_output_layer): Linear(in_features=32, out_features=29245, bias=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create torch dataset class\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SelfSupervisedTextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenized_text, example_length):\n",
    "        \"\"\"\n",
    "        Dataset to process text examples constructed with self-supervision.\n",
    "\n",
    "        Args:\n",
    "            tokenized_text (string): list of tokens to construct examples\n",
    "            example_length (int): length of inputs strings for model\n",
    "        \"\"\"\n",
    "\n",
    "        # we divide tokenized text into subsequences of (equal) example_length\n",
    "        # we ignore leftover tokens at the end\n",
    "        self._examples = []\n",
    "\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "        for i in range(0, len(tokenized_text), example_length):\n",
    "            self._examples.append(tokenized_text[i:i + example_length])\n",
    "        if len(self._examples[-1]) < example_length:\n",
    "               self._examples.pop()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self._examples[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 16824\n",
      "Example length: 64\n"
     ]
    }
   ],
   "source": [
    "# create shakespeare dataset\n",
    "training_dataset = SelfSupervisedTextDataset(shakespeare_splits_ids[\"train\"], \n",
    "                                             max_input_length)\n",
    "print(\"Num examples:\", len(training_dataset))\n",
    "print(\"Example length:\", len(training_dataset[0]))\n",
    "# output should be:\n",
    "# Num examples: 16824\n",
    "# Example length: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Function to construct labeled examples from given batch.\n",
    "\n",
    "    Args:\n",
    "        batch (tensor): tensor of size batch_size x sequence_length with tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # we create two lists for our training examples: inputs and corresponding \n",
    "    # targets    \n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    for example in batch:\n",
    "        inputs.append(torch.tensor(example[:-1]))\n",
    "        targets.append(torch.tensor(example[-1]))\n",
    "\n",
    "    return torch.stack(inputs, dim=0), torch.stack(targets, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1, 2, 3],\n",
      "        [5, 6, 7]]), tensor([4, 8]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17987/197982247.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs.append(torch.tensor(example[:-1]))\n",
      "/tmp/ipykernel_17987/197982247.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets.append(torch.tensor(example[-1]))\n"
     ]
    }
   ],
   "source": [
    "# test your collate_fn\n",
    "# we create a toy batch of sequence_length = 4\n",
    "toy_batch = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "print(collate_fn(toy_batch))\n",
    "# output should be: \n",
    "# (tensor([[1, 2, 3],\n",
    "#          [5, 6, 7]]), tensor([4, 8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x73d3a495ae20>\n"
     ]
    }
   ],
   "source": [
    "# test your dataloader\n",
    "batch_size = 128\n",
    "training_dataloader = DataLoader(training_dataset, \n",
    "                                 collate_fn=collate_fn,\n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=True, \n",
    "                                 num_workers=0)\n",
    "print(training_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write training loop\n",
    "import time, math\n",
    "\n",
    "def train(model, dataloader, num_epochs=10, print_batch_stats=False, rnn=False):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "\n",
    "    Args:\n",
    "        model: some LM implemented in PyTorch\n",
    "        dataloader: dataloader that returns sentences as examples\n",
    "        num_epochs (int): number of epochs to train \n",
    "    \"\"\"\n",
    "\n",
    "    # set training hyperparameters\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    learning_rate = 0.1  # warm up and scheduler are tricks for this\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    # we iterate over epochs\n",
    "    num_batches = len(dataloader)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.\n",
    "        start_time = time.time()\n",
    "\n",
    "        # we iterate over batches\n",
    "        for batch_num, batch in enumerate(dataloader):\n",
    "\n",
    "            # get inputs and targets from batch\n",
    "            inputs = batch[0].to(DEVICE)\n",
    "            targets = batch[1].to(DEVICE)\n",
    "\n",
    "            # forward pass\n",
    "            if not rnn:\n",
    "                output = model(inputs)\n",
    "                loss_value = loss_fn(output, targets)\n",
    "            else:\n",
    "                hidden = model.init_hidden(len(inputs))\n",
    "                output, hidden = model(inputs, hidden)\n",
    "                # output is [batch_size, input_length, embedding_size)\n",
    "                # targets is [batch_size, input_length]\n",
    "                # we need to match both to compute the loss\n",
    "                loss_value = loss_fn(\n",
    "                    output.view(targets.size()[0] * targets.size()[1], -1), \n",
    "                    targets.view(-1)\n",
    "                )\n",
    "\n",
    "            # backward pass\n",
    "            # PyTorch sums up gradients that are computed in sequence\n",
    "            # so unless we \"erase\" those gradients after every update, \n",
    "            # we will backpropagate through different batches\n",
    "            # so we set them to zero every time\n",
    "            # higher level libraries will allow you to control how often this\n",
    "            # zero_grad function is used with the parameters accumulate_grads\n",
    "            # e.g. accumulate_grads = 3 means that we zero_grad every 3 batches\n",
    "            optimizer.zero_grad()\n",
    "            # here we compute gradients\n",
    "            loss_value.backward()\n",
    "            # here we update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss_value.item()\n",
    "\n",
    "            # log batch stats    \n",
    "            if print_batch_stats:\n",
    "                print(f\"| Batch {batch_num+1:6d}/{num_batches:6d} \"\n",
    "                      f\"| Loss {loss_value:6.4f} \"\n",
    "                      f\"| Batch PPL {math.exp(loss_value):8.2f}\")\n",
    "        \n",
    "        # compute avg loss per batch\n",
    "        avg_loss = total_loss / num_batches\n",
    "        \n",
    "        # compute perplexity where avg loss is likelihood (empirical risk)\n",
    "        ppl = math.exp(avg_loss)\n",
    "\n",
    "        # compute epoch time\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        # log epoch stats\n",
    "        print(\n",
    "            f\"| Epoch {epoch+1:2d}/{num_epochs:2d} | Epoch Time {epoch_time:5f} \"\n",
    "            f\"| Avg Loss {avg_loss:6.4f} | PPL {ppl:8.2f}\"\n",
    "        )\n",
    "\n",
    "        # reset total loss and timer\n",
    "        total_loss = 0.\n",
    "        start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch  1/20 | Epoch Time 4.976456 | Avg Loss 7.3138 | PPL  1500.85\n",
      "| Epoch  2/20 | Epoch Time 4.854805 | Avg Loss 6.0851 | PPL   439.25\n",
      "| Epoch  3/20 | Epoch Time 4.461584 | Avg Loss 5.5492 | PPL   257.04\n",
      "| Epoch  4/20 | Epoch Time 4.515143 | Avg Loss 5.1215 | PPL   167.59\n",
      "| Epoch  5/20 | Epoch Time 4.641646 | Avg Loss 4.7727 | PPL   118.24\n",
      "| Epoch  6/20 | Epoch Time 4.664999 | Avg Loss 4.4762 | PPL    87.90\n",
      "| Epoch  7/20 | Epoch Time 4.635690 | Avg Loss 4.2159 | PPL    67.76\n",
      "| Epoch  8/20 | Epoch Time 4.476098 | Avg Loss 3.9929 | PPL    54.21\n",
      "| Epoch  9/20 | Epoch Time 4.640388 | Avg Loss 3.8011 | PPL    44.75\n",
      "| Epoch 10/20 | Epoch Time 4.678479 | Avg Loss 3.6323 | PPL    37.80\n",
      "| Epoch 11/20 | Epoch Time 4.674920 | Avg Loss 3.4835 | PPL    32.57\n",
      "| Epoch 12/20 | Epoch Time 4.509139 | Avg Loss 3.3527 | PPL    28.58\n",
      "| Epoch 13/20 | Epoch Time 4.645756 | Avg Loss 3.2357 | PPL    25.42\n",
      "| Epoch 14/20 | Epoch Time 4.699732 | Avg Loss 3.1299 | PPL    22.87\n",
      "| Epoch 15/20 | Epoch Time 4.645321 | Avg Loss 3.0365 | PPL    20.83\n",
      "| Epoch 16/20 | Epoch Time 4.586313 | Avg Loss 2.9474 | PPL    19.06\n",
      "| Epoch 17/20 | Epoch Time 4.531873 | Avg Loss 2.8658 | PPL    17.56\n",
      "| Epoch 18/20 | Epoch Time 4.753127 | Avg Loss 2.7902 | PPL    16.28\n",
      "| Epoch 19/20 | Epoch Time 4.757832 | Avg Loss 2.7241 | PPL    15.24\n",
      "| Epoch 20/20 | Epoch Time 4.699395 | Avg Loss 2.6588 | PPL    14.28\n"
     ]
    }
   ],
   "source": [
    "# test your training loop (loss and ppl should go down)\n",
    "train(neural_lm_1, training_dataloader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate \n",
    "def evaluate(model, dataloader, print_batch_stats=False, rnn=False):\n",
    "    \"\"\"\n",
    "    Evaluate model on given dataset.\n",
    "\n",
    "    Args:\n",
    "        model: some LM implemented in PyTorch\n",
    "        dataloader: dataloader that returns sentences as examples\n",
    "    \"\"\"\n",
    "\n",
    "    # we use cross entropy so we can compute perplexity from this\n",
    "    # we sum loss up, to then divide by number of examples\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    # set model to eval mode (turns off dropout, etc.)\n",
    "    model.eval()\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "    num_examples = 0\n",
    "    total_loss = 0.\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # we iterate over batches\n",
    "        start_time = time.time()\n",
    "        for batch_num, (inputs, targets) in enumerate(dataloader):\n",
    "\n",
    "            # move inputs and targets to device\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "\n",
    "            # add up number of examples\n",
    "            num_examples += len(inputs)\n",
    "\n",
    "            # compute loss\n",
    "            if not rnn:\n",
    "                output = model(inputs)\n",
    "                loss_value = loss_fn(output, targets)\n",
    "            else:\n",
    "                hidden = model.init_hidden(len(inputs))\n",
    "                output, hidden = model(inputs, hidden)\n",
    "                # output is [batch_size, input_length, embedding_size)\n",
    "                # targets is [batch_size]\n",
    "                # we want only predictions for last word\n",
    "                loss_value = loss_fn(\n",
    "                    output[:, -1, :],\n",
    "                    targets.view(-1)\n",
    "                )\n",
    "\n",
    "            # add up loss\n",
    "            total_loss += loss_value.item()\n",
    "\n",
    "            # log batch stats    \n",
    "            if print_batch_stats:\n",
    "                print(f\"| Batch {batch_num+1:6d}/{num_batches:6d} \"\n",
    "                        f\"| Loss {loss_value:6.4f} \"\n",
    "                        f\"| Batch PPL {math.exp(loss_value):8.2f}\")\n",
    "        \n",
    "        # compute avg loss per batch\n",
    "        avg_loss = total_loss / num_examples\n",
    "        \n",
    "        # compute perplexity where avg loss is likelihood (empirical risk)\n",
    "        ppl = math.exp(avg_loss)\n",
    "\n",
    "        # compute epoch time\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        # log epoch stats\n",
    "        print(\n",
    "            f\"| Run Time {total_time:5f} \"\n",
    "            f\"| Avg Loss {avg_loss:6.4f} \"\n",
    "            f\"| PPL {ppl:8.2f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.SelfSupervisedTextDataset object at 0x73d360cd7f40>\n"
     ]
    }
   ],
   "source": [
    "# create validation dataset\n",
    "validation_dataset = SelfSupervisedTextDataset(shakespeare_splits_ids[\"valid\"], \n",
    "                                               max_input_length)\n",
    "print(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader for validation\n",
    "batch_size = 128 \n",
    "validation_dataloader = DataLoader(validation_dataset, \n",
    "                                 collate_fn=collate_fn,\n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=True, \n",
    "                                 num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Run Time 0.159164 | Avg Loss 8.7676 | PPL  6422.75\n"
     ]
    }
   ],
   "source": [
    "# evaluate FNN\n",
    "evaluate(neural_lm_1, validation_dataloader, rnn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Language Models with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now an RNN-based language model\n",
    "\n",
    "class RNNLM(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocabulary_size, \n",
    "                 embedding_size,\n",
    "                 hidden_layer_size,\n",
    "                 tie_weights=True,\n",
    "                 num_hidden_layers=1,\n",
    "                 hidden_layer_activation=\"tanh\",\n",
    "                 dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        RNN-based language model.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self._num_layers = num_hidden_layers\n",
    "        self._hidden_layer_size = hidden_layer_size\n",
    "        self._embedding_size = embedding_size\n",
    "\n",
    "        self._embeddings = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self._rnn = nn.RNN(embedding_size, \n",
    "                           hidden_layer_size,\n",
    "                           num_layers=num_hidden_layers, \n",
    "                           dropout=dropout_rate, \n",
    "                           nonlinearity=hidden_layer_activation,\n",
    "                           batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_layer_size, vocabulary_size)\n",
    "\n",
    "        if tie_weights:\n",
    "            self._embeddings.weight = self.output_layer.weight\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize hidden states.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): batch size\n",
    "        \"\"\"\n",
    "\n",
    "        hidden = torch.zeros(self._num_layers, \n",
    "                             batch_size, \n",
    "                             self._hidden_layer_size)\n",
    "\n",
    "        return hidden.to(DEVICE)\n",
    "\n",
    "    def forward(self, input_ids, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            input_ids (tensor): 3D tensors with batched data, or 2D tensor\n",
    "            hidden (tensor): tensor with initial hidden states\n",
    "\n",
    "        Return:\n",
    "            out (tensor): tensor of size [batch size, seq length, vocab_size]\n",
    "        \"\"\"\n",
    "\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "        # embed input sequence (same as forward pass in NeuralLM)        \n",
    "        embs = self._embeddings(\n",
    "            input_ids.view(-1)).view(input_ids.size()[0], \n",
    "                                     input_ids.size()[1], \n",
    "                                     -1)\n",
    "        # compute hidden states\n",
    "        out, hidden_state = self._rnn(embs, hidden)          \n",
    "        # compute output\n",
    "        out = self.output_layer(out)\n",
    "        \n",
    "        return out, hidden_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNLM(\n",
      "  (_embeddings): Embedding(29245, 16)\n",
      "  (_rnn): RNN(16, 16, batch_first=True)\n",
      "  (output_layer): Linear(in_features=16, out_features=29245, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# test your RNN\n",
    "embedding_size = 16\n",
    "hidden_layer_size = 16\n",
    "rnn_lm_1 = RNNLM(\n",
    "    len(shakespeare_vocab),\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    ").to(DEVICE)\n",
    "print(rnn_lm_1)\n",
    "\n",
    "# output should be:\n",
    "# RNNLM(\n",
    "#   (_embeddings): Embedding(29245, 16)\n",
    "#   (_rnn): RNN(16, 16, batch_first=True)\n",
    "#   (output_layer): Linear(in_features=16, out_features=29245, bias=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a slightly different collate function for the RNN, if we want to train \n",
    "# with teacher forcing\n",
    "def rnn_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Function to construct labeled example from given batch.\n",
    "\n",
    "    Args:\n",
    "        batch (tensor): tensor of size batch_size x sentence_length with tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # we create two lists for our training examples: inputsand corresponding \n",
    "    # targets    \n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    for example in batch:\n",
    "        # RNN\n",
    "        inputs.append(torch.tensor(example[:-1]))\n",
    "        targets.append(torch.tensor(example[1:]))\n",
    "\n",
    "    return torch.stack(inputs, dim=0), torch.stack(targets, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1, 2, 3],\n",
      "        [5, 6, 7]]), tensor([[2, 3, 4],\n",
      "        [6, 7, 8]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17987/729527823.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs.append(torch.tensor(example[:-1]))\n",
      "/tmp/ipykernel_17987/729527823.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets.append(torch.tensor(example[1:]))\n"
     ]
    }
   ],
   "source": [
    "# test your collate_fn\n",
    "# we create a toy batch of sequence_length = 4\n",
    "toy_batch = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "print(rnn_collate_fn(toy_batch))\n",
    "\n",
    "# output should be:\n",
    "# (tensor([[1, 2, 3],\n",
    "#          [5, 6, 7]]), \n",
    "#  tensor([[2, 3, 4],\n",
    "#          [6, 7, 8]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x73d36038b310>\n"
     ]
    }
   ],
   "source": [
    "# create a new dataloader with this new collate function\n",
    "rnn_training_dataloader = DataLoader(training_dataset, \n",
    "                                 collate_fn=rnn_collate_fn,\n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=True, \n",
    "                                 num_workers=0)\n",
    "print(rnn_training_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch  1/ 1 | Epoch Time 343.757031 | Avg Loss 6.1271 | PPL   458.09\n"
     ]
    }
   ],
   "source": [
    "# train your RNN (DO NOT RUN THIS DURING CLASS!)\n",
    "train(rnn_lm_1, rnn_training_dataloader, num_epochs=1, rnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Run Time 3.508857 | Avg Loss 5.8659 | PPL   352.81\n"
     ]
    }
   ],
   "source": [
    "# evaluate RNN\n",
    "# we use the same dataloader as with our NeuralLM model, so performance is \n",
    "# comparable, but the evaluate function must be able to handle both models\n",
    "evaluate(rnn_lm_1, validation_dataloader, rnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct reverse dict to decode text generated with our RNN\n",
    "reverse_shapeskeare_vocab = {}\n",
    "for k, v in shakespeare_vocab.items():\n",
    "    reverse_shapeskeare_vocab[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text with RNN\n",
    "def generate_text_with_rnn(num_tokens, \n",
    "                           context, \n",
    "                           model, \n",
    "                           temperature, \n",
    "                           decoding_dict):\n",
    "    \"\"\"\n",
    "    Generate num_tokens given context and model.\n",
    "\n",
    "    Args:\n",
    "        num_tokens (int): number of tokens to be generated\n",
    "        context (tensor): sequence of n token IDs in tensor of size [1, n]\n",
    "        model (RNNLM): RNN model\n",
    "        temperature (float): softmax temperature\n",
    "        decoding_dict (dict): dict of the form {token_id: token}\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "    context = torch.unsqueeze(context, 0)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "        for _ in range(num_tokens):\n",
    "            # batch size = 1 in this case\n",
    "            hidden = model.init_hidden(1)\n",
    "            output, hidden = model(context, hidden)\n",
    "            # output is [batch_size, input_length, embedding_size)\n",
    "            # we want only predictions for last word\n",
    "            # so we apply softmax to logits of output corresponding to last word\n",
    "            probs = torch.softmax(output[:, -1, :] / temperature, dim=-1)\n",
    "            # get next word prediction from model's softmax distribution\n",
    "            next_word = torch.multinomial(probs, num_samples=1).item()\n",
    "            predictions.append(next_word)\n",
    "            # to ensure autoregressive generation, we add the predicted word\n",
    "            # to the context\n",
    "            context = torch.cat(\n",
    "                (context, torch.tensor([next_word]).unsqueeze(0)), \n",
    "                dim=1\n",
    "                )\n",
    "\n",
    "    # decode predictions\n",
    "    output_tokens = []\n",
    "    for pred in predictions:\n",
    "        output_tokens.append(decoding_dict[pred])\n",
    "\n",
    "    return \" \".join(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "we\n",
      "desire\n",
      "\n",
      "GENERATED TEXT:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\", and , I will be the ' d , and a good , and a lord . I have\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test text generation\n",
    "\n",
    "# construct context\n",
    "context_ids = [10, 11]\n",
    "# context_ids = [110]\n",
    "context = torch.tensor(context_ids)\n",
    "# print context\n",
    "print(\"PROMPT:\")\n",
    "for id in context_ids:\n",
    "    print(reverse_shapeskeare_vocab[id])\n",
    "print()\n",
    "\n",
    "# generate!\n",
    "# play around with temperature, higher values make distribution more uniform\n",
    "# lower values puts more mass on already probably events\n",
    "num_tokens = 20\n",
    "temperature = .2\n",
    "print(\"GENERATED TEXT:\")\n",
    "generate_text_with_rnn(num_tokens, \n",
    "                       context, \n",
    "                       rnn_lm_1, \n",
    "                       temperature, \n",
    "                       reverse_shapeskeare_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a modified version of the generated_text_with_rnn function. It now\n",
    "# takes as input a sample function and a value for k.\n",
    "def generate_text_with_rnn_2(num_tokens, \n",
    "                           context, \n",
    "                           model, \n",
    "                           temperature, \n",
    "                           decoding_dict,\n",
    "                           sampling_fn,\n",
    "                           k):\n",
    "    \"\"\"\n",
    "    Generate num_tokens given context and model.\n",
    "\n",
    "    Args:\n",
    "        num_tokens (int): number of tokens to be generated\n",
    "        context (tensor): sequence of n token IDs in tensor of size [1, n]\n",
    "        model (RNNLM): RNN model\n",
    "        temperature (float): softmax temperature\n",
    "        decoding_dict (dict): dict of the form {token_id: token}\n",
    "        sampling_fn (callable): function to produce single sample given \n",
    "                                distribution\n",
    "        k (int): number of top-k elements in distribution to sample from\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "    context = torch.unsqueeze(context, 0)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "        for _ in range(num_tokens):\n",
    "            # batch size = 1 in this case\n",
    "            hidden = model.init_hidden(1)\n",
    "            output, hidden = model(context, hidden)\n",
    "            # output is [batch_size, input_length, embedding_size)\n",
    "            # we want only predictions for last word\n",
    "            # we don't apply softmax here, as the sample function will do that\n",
    "            logits = output[:, -1, :]\n",
    "            # get next word prediction using given sample function\n",
    "            next_word = sampling_fn(logits, k, temperature)\n",
    "            predictions.append(next_word)\n",
    "            # to ensure autoregressive generation, we add the predicted word\n",
    "            # to the context\n",
    "            context = torch.cat(\n",
    "                (context, torch.tensor([next_word]).unsqueeze(0)), \n",
    "                dim=1\n",
    "                )\n",
    "\n",
    "    # decode predictions\n",
    "    output_tokens = []\n",
    "    for pred in predictions:\n",
    "        output_tokens.append(decoding_dict[pred])\n",
    "\n",
    "    return \" \".join(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-k sampling (gives us greedy with k = 1 and random with k = |V|)\n",
    "def topk_sampling(logits, k, temperature):\n",
    "    \"\"\"\n",
    "    Top-k sampling, we get greedy sampling with k = 1 and random sampling with \n",
    "    k = |V|.\n",
    "\n",
    "    Args:\n",
    "        logits (tensor): tensor of unnormalized probabilities\n",
    "        k (int): number of top-k elements in distribution to sample from\n",
    "        temperature (float): softmax temperature\n",
    "    \"\"\"\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    # get top-k elements\n",
    "    topk = torch.topk(logits, k)\n",
    "    # turn top-k elements into normalized distribution\n",
    "    probs = torch.softmax(topk.values / temperature, dim=-1)\n",
    "\n",
    "    # retun sample from top-k distribution\n",
    "    return torch.multinomial(probs, num_samples=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "we\n",
      "desire\n",
      "\n",
      "GENERATED TEXT:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"should SONNETS 1609 where 1609 thy thy Thou From 1 1609 fairest contracted ' s fairest SONNETS 1 1609 THE\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test text generation now with different sampling approaches\n",
    "\n",
    "# construct context\n",
    "context_ids = [10, 11]\n",
    "# context_ids = [10, 11, 110]\n",
    "# context_ids = [11]\n",
    "# context_ids = [110]\n",
    "context = torch.tensor(context_ids)\n",
    "# print context\n",
    "print(\"PROMPT:\")\n",
    "for id in context_ids:\n",
    "    print(reverse_shapeskeare_vocab[id])\n",
    "print()\n",
    "\n",
    "# generate!\n",
    "# in addition to temperature, play around with different values of k\n",
    "num_tokens = 20\n",
    "temperature = 0.9\n",
    "k = 100\n",
    "print(\"GENERATED TEXT:\")\n",
    "generate_text_with_rnn_2(num_tokens, \n",
    "                       context, \n",
    "                       rnn_lm_1, \n",
    "                       temperature, \n",
    "                       reverse_shapeskeare_vocab,\n",
    "                       topk_sampling,\n",
    "                       k\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNLM(\n",
      "  (_embeddings): Embedding(29245, 16)\n",
      "  (_rnn): RNN(16, 16, batch_first=True)\n",
      "  (output_layer): Linear(in_features=16, out_features=29245, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# test your RNN\n",
    "embedding_size = 16\n",
    "hidden_layer_size = 16\n",
    "rnn1 = RNNLM(\n",
    "    len(shakespeare_vocab),\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    ").to(DEVICE)\n",
    "print(rnn1)\n",
    "\n",
    "# output should be:\n",
    "# RNNLM(\n",
    "#   (_embeddings): Embedding(29245, 16)\n",
    "#   (_rnn): RNN(16, 16, batch_first=True)\n",
    "#   (output_layer): Linear(in_features=16, out_features=29245, bias=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch  1/20 | Epoch Time 5.486800 | Avg Loss 6.1755 | PPL   480.84\n",
      "| Epoch  2/20 | Epoch Time 5.453655 | Avg Loss 5.6731 | PPL   290.92\n",
      "| Epoch  3/20 | Epoch Time 5.457234 | Avg Loss 5.5263 | PPL   251.22\n",
      "| Epoch  4/20 | Epoch Time 5.454179 | Avg Loss 5.4471 | PPL   232.09\n",
      "| Epoch  5/20 | Epoch Time 5.429858 | Avg Loss 5.3950 | PPL   220.31\n",
      "| Epoch  6/20 | Epoch Time 5.437742 | Avg Loss 5.3579 | PPL   212.28\n",
      "| Epoch  7/20 | Epoch Time 5.460291 | Avg Loss 5.3294 | PPL   206.32\n",
      "| Epoch  8/20 | Epoch Time 5.425132 | Avg Loss 5.3069 | PPL   201.72\n",
      "| Epoch  9/20 | Epoch Time 5.442552 | Avg Loss 5.2886 | PPL   198.06\n",
      "| Epoch 10/20 | Epoch Time 5.456953 | Avg Loss 5.2739 | PPL   195.18\n",
      "| Epoch 11/20 | Epoch Time 5.462910 | Avg Loss 5.2597 | PPL   192.42\n",
      "| Epoch 12/20 | Epoch Time 5.552658 | Avg Loss 5.2494 | PPL   190.46\n",
      "| Epoch 13/20 | Epoch Time 5.555686 | Avg Loss 5.2393 | PPL   188.54\n",
      "| Epoch 14/20 | Epoch Time 5.550479 | Avg Loss 5.2301 | PPL   186.81\n",
      "| Epoch 15/20 | Epoch Time 5.534641 | Avg Loss 5.2223 | PPL   185.36\n",
      "| Epoch 16/20 | Epoch Time 5.429454 | Avg Loss 5.2152 | PPL   184.05\n",
      "| Epoch 17/20 | Epoch Time 5.456941 | Avg Loss 5.2088 | PPL   182.88\n",
      "| Epoch 18/20 | Epoch Time 5.461945 | Avg Loss 5.2026 | PPL   181.75\n",
      "| Epoch 19/20 | Epoch Time 5.464255 | Avg Loss 5.1963 | PPL   180.60\n",
      "| Epoch 20/20 | Epoch Time 5.444581 | Avg Loss 5.1912 | PPL   179.69\n"
     ]
    }
   ],
   "source": [
    "# let's train longer to check convergence\n",
    "train(rnn1, rnn_training_dataloader, num_epochs=20, rnn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Run Time 0.056986 | Avg Loss 5.8776 | PPL   356.96\n"
     ]
    }
   ],
   "source": [
    "# evaluate RNN\n",
    "evaluate(rnn1, validation_dataloader, rnn=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
