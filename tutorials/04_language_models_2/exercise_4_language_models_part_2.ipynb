{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Methods in Text Analytics\n",
    "# Exercise 4: Language Models - Part 2\n",
    "### Daniel Ruffinelli\n",
    "## FSS 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. N-Gram Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a simple whitespace tokenizer that removes punctuation\n",
    "def tokenize(text):\n",
    "    \"\"\" \n",
    "    Given text, returns all words separated by white space after separating all\n",
    "    punctuation.\n",
    "\n",
    "    Args:\n",
    "        text: string with text to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list of tokens\n",
    "    \"\"\"\n",
    "\n",
    "    import string\n",
    "\n",
    "    # separate punctuation symbols with whitespaces\n",
    "    for symbol in string.punctuation:\n",
    "        text = text.replace(symbol, \" \" + symbol + \" \")\n",
    "    text_split = text.split()\n",
    "\n",
    "    return text_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we test the tokenizer\n",
    "text = \"This is a phrase, with some punctuation, that we want to tokenize!\"\n",
    "print(tokenize(text))\n",
    "# output should be:\n",
    "# ['This', 'is', 'a', 'phrase', ',', 'with', 'some', 'punctuation', ',', 'that', 'we', 'want', 'to', 'tokenize', '!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for counting n-grams\n",
    "def compute_ngrams(n, tokenized_text):\n",
    "    \"\"\" \n",
    "    Compute n-grams in given list of tokens.\n",
    "\n",
    "    Args:\n",
    "        n: size of n-grams\n",
    "        tokens: list of tokens\n",
    "\n",
    "    Returns\n",
    "        list of n-grams of the form (context, next_word), where context is a \n",
    "        tuple of n -1 previous words.\n",
    "    \"\"\"\n",
    "\n",
    "    # we add left-side padding (i.e. start of sentence symbols)\n",
    "    tokens = (n-1)*[\"<s>\"] + tokenized_text\n",
    "\n",
    "    # list to store ngrams\n",
    "    ngrams = []\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your ngram computation\n",
    "text = \"This is a phrase, with some punctuation, that we want to tokenize!\"\n",
    "print(compute_ngrams(3, tokenize(text)))\n",
    "# output should be:\n",
    "# [(('<s>', '<s>'), 'This'), (('<s>', 'This'), 'is'), (('This', 'is'), 'a'), (('is', 'a'), 'phrase'), (('a', 'phrase'), ','), (('phrase', ','), 'with'), ((',', 'with'), 'some'), (('with', 'some'), 'punctuation'), (('some', 'punctuation'), ','), (('punctuation', ','), 'that'), ((',', 'that'), 'we'), (('that', 'we'), 'want'), (('we', 'want'), 'to'), (('want', 'to'), 'tokenize'), (('to', 'tokenize'), '!')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an n-gram model that keeps track of \"context to next word\" pairs\n",
    "# and an ngram counter\n",
    "from collections import defaultdict as ddict \n",
    "\n",
    "class NGramModel:\n",
    "    \"\"\"\n",
    "    NGramModel class, keeps track of (context, next_word) pairs and an ngram\n",
    "    counter.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "\n",
    "        self._n = n\n",
    "\n",
    "        # set to store vocabulary\n",
    "        self.vocabulary = set()\n",
    "\n",
    "        # dict to store ngram counters\n",
    "        self.ngram_counter = ddict(int)\n",
    "\n",
    "        # dict to track (context, next_word) pairs\n",
    "        self.context = ddict(list)\n",
    "\n",
    "    def extend_vocabulary(self, tokenized_text):\n",
    "        \"\"\" \n",
    "        Adds tokens to vocabulary. Useful for smoothing for unseen data.\n",
    "        \"\"\"\n",
    "        for token in tokenized_text:\n",
    "            self.vocabulary.add(token)\n",
    "\n",
    "    def update(self, tokenized_text):\n",
    "        \"\"\" \n",
    "        Updates counts of ngram model by computing ngrams in given tokenized \n",
    "        text and adding them to ngram counter and context tracker.\n",
    "\n",
    "        Args:\n",
    "            text (str): text to tokenize and compute ngrams\n",
    "        \"\"\"\n",
    "\n",
    "        # compute ngrams\n",
    "        ngrams = compute_ngrams(self._n, tokenized_text)\n",
    "\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "    def next_word_prob(self, context, next_word):\n",
    "        \"\"\"\n",
    "        Returns probability of predicting next_word after given context.\n",
    "        We use add-1 smoothing to process unseen words.\n",
    "\n",
    "        Args:\n",
    "            context (list of tokens): context words\n",
    "            next_word (str): next word\n",
    "\n",
    "        Returns:\n",
    "            prob (float): probability of next_word following context.\n",
    "        \"\"\"\n",
    "\n",
    "        # probability value to return\n",
    "        prob = 0.\n",
    "\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your NGram model\n",
    "bigram_model = NGramModel(2)\n",
    "print(\"Size on n-gram:\", bigram_model._n)\n",
    "# output should be: Size on n-gram: 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your update function\n",
    "bigram_model.update(tokenize(\"We now need text! Let's add some text to our bigram model\"))\n",
    "print(\"Vocabulary:\", bigram_model.vocabulary)\n",
    "print(\"N-Gram Counter:\", bigram_model.ngram_counter)\n",
    "print(\"Context Dict:\", bigram_model.context)\n",
    "# output should be:\n",
    "# Vocabulary: {'s', '!', 'model', 'to', 'Let', 'add', \"'\", 'bigram', 'need', 'now', 'We', 'text', 'our', 'some'}\n",
    "# N-Gram Counter: defaultdict(<class 'int'>, {(('<s>',), 'We'): 1, (('We',), 'now'): 1, (('now',), 'need'): 1, (('need',), 'text'): 1, (('text',), '!'): 1, (('!',), 'Let'): 1, (('Let',), \"'\"): 1, ((\"'\",), 's'): 1, (('s',), 'add'): 1, (('add',), 'some'): 1, (('some',), 'text'): 1, (('text',), 'to'): 1, (('to',), 'our'): 1, (('our',), 'bigram'): 1, (('bigram',), 'model'): 1})\n",
    "# Context Dict: defaultdict(<class 'list'>, {('<s>',): ['We'], ('We',): ['now'], ('now',): ['need'], ('need',): ['text'], ('text',): ['!', 'to'], ('!',): ['Let'], ('Let',): [\"'\"], (\"'\",): ['s'], ('s',): ['add'], ('add',): ['some'], ('some',): ['text'], ('to',): ['our'], ('our',): ['bigram'], ('bigram',): ['model']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your next_word_prob function\n",
    "print(\"Probability that word 'to' follows word 'text':\", bigram_model.next_word_prob((\"text\",), \"to\"))\n",
    "# output should be:\n",
    "# Probability that word 'to' follows word 'text': 0.125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to predict next word given context\n",
    "def predict_next_word(context, model):\n",
    "    \"\"\"\n",
    "    Uses given model to predict the next word that could follow the given \n",
    "    context. Each possible next word is chosen based on its probability given\n",
    "    by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    import random\n",
    "\n",
    "    # get all possible next words\n",
    "    next_words = model.context[context]\n",
    "    next_words_probs = []\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    # select one of possible next word to predict\n",
    "    predicted_word = random.choices(next_words, next_words_probs, k=1)\n",
    "\n",
    "    return predicted_word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your function predict_next_word\n",
    "predict_next_word((\"text\",), bigram_model)\n",
    "# output should be either '!' or 'to'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load shakespeare's entire works\n",
    "with open(\"shakespeare_train.txt\") as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "# break it into sentences\n",
    "corpus_sentences = corpus.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a trigram model\n",
    "trigram_model = NGramModel(3)\n",
    "\n",
    "# feed corpus to model\n",
    "for sentence in corpus_sentences:\n",
    "        trigram_model.update(tokenize(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check most common ngrams\n",
    "top = 10\n",
    "sorted(trigram_model.ngram_counter.items(), key=lambda x:x[1], reverse=True)[:top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define function to generate n tokens given starting token\n",
    "def generate_text(num_tokens, context, model):\n",
    "    \"\"\"\n",
    "    Generate num_tokens words using given model\n",
    "\n",
    "    Args:\n",
    "        num_tokens (int): number of tokens to generate\n",
    "        context (tuple of previous words): starting prompt\n",
    "        model (NGramModel): ngram model\n",
    "    \"\"\"\n",
    "\n",
    "    output = []\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next words\n",
    "context = (\"My\", \"lord\")\n",
    "\n",
    "# test text generation\n",
    "# this may fail at times because our simple model does not have an entry for the\n",
    "# given context in its {context: next_word} dict, which in turn happens because\n",
    "# the generated context (prompt) is a combination the model has not seen\n",
    "# In short, it's a problem of our implementation that uses a dict\n",
    "# In practice, it's not often done like this (we'll see in the next tasks)\n",
    "print(generate_text(40, context, trigram_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try stronger models\n",
    "fourgram_model = NGramModel(4)\n",
    "\n",
    "# feed corpus to model\n",
    "# fourgram_model.update(tokenize(corpus))\n",
    "for sentence in corpus_sentences:\n",
    "        fourgram_model.update(tokenize(sentence))\n",
    "\n",
    "# check most common ngrams\n",
    "top = 10\n",
    "sorted(fourgram_model.ngram_counter.items(), key=lambda x:x[1], reverse=True)[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next words\n",
    "context = (\"I\", \"'\", \"ll\")\n",
    "\n",
    "# test text generation\n",
    "print(generate_text(20, context, fourgram_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and a 5-gram models\n",
    "fivegram_model = NGramModel(5)\n",
    "\n",
    "# feed corpus to model\n",
    "# fivegram_model.update(tokenize(corpus))\n",
    "for sentence in corpus_sentences:\n",
    "        fivegram_model.update(tokenize(sentence))\n",
    "\n",
    "# check most common ngrams\n",
    "top = 10\n",
    "sorted(fivegram_model.ngram_counter.items(), key=lambda x:x[1], reverse=True)[:top]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next words\n",
    "context = (\"<s>\", \"<s>\", \"<s>\", \"<s>\")\n",
    "\n",
    "# test text generation\n",
    "print(generate_text(20, context, fivegram_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute likelihood of validation data\n",
    "def compute_likelihood(tokenized_text, model, n):\n",
    "    \"\"\"\n",
    "    Compute log_likelihood of given tokenized text using given model.\n",
    "    We compute likelihood in log space to prevent numerical underflow.\n",
    "\n",
    "    Args:\n",
    "        tokenized_text (list of tokens): text to compute likelihood of\n",
    "        model (NGramModel): model used to compute likelihood\n",
    "        n (int): size of n-grams of givem model\n",
    "    \"\"\"\n",
    "\n",
    "    import math\n",
    "\n",
    "    log_likelihood = 0.0\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    return log_likelihood, math.exp(-log_likelihood / i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load shakespeare's validation data\n",
    "with open(\"shakespeare_valid.txt\") as f:\n",
    "    corpus_valid = f.read()\n",
    "\n",
    "# tokenize it\n",
    "tokenized_validation = tokenize(corpus_valid)\n",
    "\n",
    "print(tokenized_validation[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend trigram model vocabulary for proper smoothing\n",
    "trigram_model.extend_vocabulary(tokenized_validation)\n",
    "\n",
    "#  compute validation likelihood\n",
    "valid_likelihood, perplexity = compute_likelihood(tokenize(corpus_valid), trigram_model, 3)\n",
    "print(\"Validation likelihood:\", valid_likelihood)\n",
    "print(\"Validation perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Language Models with Fully-Connected Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# test it\n",
    "a = torch.rand(3,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's preprocess our text (we work with embeddings now, not just strings)\n",
    "from collections import defaultdict as ddict\n",
    "\n",
    "# these are our splits\n",
    "shakespeare_splits = {\n",
    "    \"train\": \"shakespeare_train.txt\", \n",
    "    \"valid\": \"shakespeare_valid.txt\", \n",
    "    \"text\": \"shakespeare_test.txt\"\n",
    "}\n",
    "\n",
    "# we create a vocabulary dict of the form {token: ID}\n",
    "shakespeare_vocab = {}\n",
    "for text_file in shakespeare_splits.values():\n",
    "    with open(text_file) as f:\n",
    "        split_text = f.read()\n",
    "        tokenized_split = tokenize(split_text)\n",
    "        for token in tokenized_split:\n",
    "            if token not in shakespeare_vocab:\n",
    "                shakespeare_vocab[token] = len(shakespeare_vocab)\n",
    "# we add the padding symbol to our vocabulary\n",
    "shakespeare_vocab[\"<s>\"] = len(shakespeare_vocab)\n",
    "print(\"Size of vocabulary:\", len(shakespeare_vocab))\n",
    "\n",
    "# we turn our splits into sequences of token IDs\n",
    "shakespeare_splits_ids = ddict(list)\n",
    "for split_id, split_file in shakespeare_splits.items():\n",
    "    with open(split_file) as f:\n",
    "            tokenized_split = tokenize(f.read())\n",
    "    for token in tokenized_split:\n",
    "        shakespeare_splits_ids[split_id].append(shakespeare_vocab[token])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralLM(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 vocabulary_size, \n",
    "                 embedding_size,\n",
    "                 max_input_length,\n",
    "                 padding_token_id,\n",
    "                 hidden_layer_sizes,\n",
    "                 hidden_layer_activation=\"tanh\"):\n",
    "        \"\"\"\n",
    "        Neural language model.\n",
    "\n",
    "        Args:\n",
    "            vocabulary_size (int): size of vocabulary\n",
    "            embedding size (int): size of input tokens\n",
    "            max_input_length (int): max. number of input tokens\n",
    "            padding_token_id (int): id of token to use for left padding\n",
    "            hidden_layer_sizes (list): list of hidden layer sizes, e.g. [10, 5]\n",
    "            hidden_layer_activation (string): activation, e.g. sigmoid, tanh\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # set hyperparameters\n",
    "        self._max_input_length = max_input_length\n",
    "        self._pad_token_id = torch.tensor(padding_token_id)\n",
    "        if hasattr(torch, hidden_layer_activation):\n",
    "            self._activation = getattr(torch, hidden_layer_activation)\n",
    "        else:\n",
    "            raise ValueError(\"Activation must be a torch-supported function.\")\n",
    "\n",
    "        # create embedding matrix\n",
    "        self._embeddings = nn.Embedding(vocabulary_size, embedding_size)\n",
    "\n",
    "        # create hidden layers    \n",
    "        self._hidden_layers = []\n",
    "        input_size = embedding_size * max_input_length\n",
    "        for output_size in hidden_layer_sizes:\n",
    "            self._hidden_layers.append(nn.Linear(input_size, output_size))\n",
    "            input_size = output_size\n",
    "        \n",
    "        # create output layer (no need for softmax, we'll do that with the loss)\n",
    "        self._output_layer = nn.Linear(output_size, vocabulary_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            seq_indices (tensor): tensor of token IDs of size \n",
    "            (batch_size, max_input_length)\n",
    "\n",
    "        Return:\n",
    "            out (tensor): tensor of size [len(input_ids), vocabulary_size]\n",
    "        \"\"\"\n",
    "\n",
    "        # pad to the left\n",
    "        num_pads = self._max_input_length - input_ids.size()[1]\n",
    "        padding_tensor = self._pad_token_id.expand(input_ids.size()[0])\n",
    "        padding_tensor = torch.unsqueeze(padding_tensor, 1).expand(-1, num_pads)\n",
    "        input_ids = torch.cat(\n",
    "            [padding_tensor, input_ids],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your NeuralLM\n",
    "embedding_size = 16\n",
    "hidden_layer_size = 32\n",
    "max_input_length = 64\n",
    "neural_lm_1 = NeuralLM(\n",
    "    len(shakespeare_vocab),\n",
    "    embedding_size=embedding_size,\n",
    "    max_input_length=max_input_length,\n",
    "    padding_token_id=shakespeare_vocab[\"<s>\"],\n",
    "    hidden_layer_sizes=[hidden_layer_size]\n",
    ")\n",
    "print(neural_lm_1)\n",
    "\n",
    "# output should be:\n",
    "# NeuralLM(\n",
    "#   (_embeddings): Embedding(29245, 16)\n",
    "#   (_output_layer): Linear(in_features=32, out_features=29245, bias=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create torch dataset class\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SelfSupervisedTextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenized_text, example_length):\n",
    "        \"\"\"\n",
    "        Dataset to process text examples constructed with self-supervision.\n",
    "\n",
    "        Args:\n",
    "            tokenized_text (string): list of tokens to construct examples\n",
    "            example_length (int): length of inputs strings for model\n",
    "        \"\"\"\n",
    "\n",
    "        # we divide tokenized text into subsequences of (equal) example_length\n",
    "        # we ignore leftover tokens at the end\n",
    "        self._examples = []\n",
    "\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self._examples[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create shakespeare dataset\n",
    "training_dataset = SelfSupervisedTextDataset(shakespeare_splits_ids[\"train\"], \n",
    "                                             max_input_length)\n",
    "print(\"Num examples:\", len(training_dataset))\n",
    "print(\"Example length:\", len(training_dataset[0]))\n",
    "# output should be:\n",
    "# Num examples: 16824\n",
    "# Example length: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Function to construct labeled example from given batch.\n",
    "\n",
    "    Args:\n",
    "        batch (tensor): tensor of size batch_size x sentence_length with tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # we create two lists for our training examples: inputs and corresponding \n",
    "    # targets    \n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    return torch.stack(inputs, dim=0), torch.stack(targets, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your collate_fn\n",
    "# we create a toy batch of sequence_length = 4\n",
    "toy_batch = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "print(collate_fn(toy_batch))\n",
    "# output should be: \n",
    "# (tensor([[1, 2, 3],\n",
    "#          [5, 6, 7]]), tensor([4, 8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your dataloader\n",
    "batch_size = 128\n",
    "training_dataloader = DataLoader(training_dataset, \n",
    "                                 collate_fn=collate_fn,\n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=True, \n",
    "                                 num_workers=0)\n",
    "print(training_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write training loop\n",
    "import time, math\n",
    "\n",
    "def train(model, dataloader, num_epochs=10, print_batch_stats=False, rnn=False):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "\n",
    "    Args:\n",
    "        model: some LM implemented in PyTorch\n",
    "        dataloader: dataloader that returns sentences as examples\n",
    "        num_epochs (int): number of epochs to train \n",
    "    \"\"\"\n",
    "\n",
    "    # set training hyperparameters\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    learning_rate = 0.1\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your training loop (loss and ppl should go down)\n",
    "train(neural_lm_1, training_dataloader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate \n",
    "def evaluate(model, dataloader, print_batch_stats=False, rnn=False):\n",
    "    \"\"\"\n",
    "    Evaluate model on given dataset.\n",
    "\n",
    "    Args:\n",
    "        model: some LM implemented in PyTorch\n",
    "        dataloader: dataloader that returns sentences as examples\n",
    "    \"\"\"\n",
    "\n",
    "    # we use cross entropy so we can compute perplexity from this\n",
    "    # we sum loss up, to then divide by number of examples\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    # set model to eval mode (turns off dropout, etc.)\n",
    "    model.eval()\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "    num_examples = 0\n",
    "    total_loss = 0.\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation dataset\n",
    "validation_dataset = SelfSupervisedTextDataset(shakespeare_splits_ids[\"valid\"], \n",
    "                                               max_input_length)\n",
    "print(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader for validation\n",
    "batch_size = 128 \n",
    "validation_dataloader = DataLoader(validation_dataset, \n",
    "                                 collate_fn=collate_fn,\n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=True, \n",
    "                                 num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate FNN\n",
    "evaluate(neural_lm_1, validation_dataloader, rnn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Language Models with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now an RNN-based language model\n",
    "# TODO: add weight sharing, see this: https://arxiv.org/pdf/1608.05859.pdf\n",
    "\n",
    "class RNNLM(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocabulary_size, \n",
    "                 embedding_size,\n",
    "                 hidden_layer_size,\n",
    "                 tie_weights=True,\n",
    "                 num_hidden_layers=1,\n",
    "                 hidden_layer_activation=\"tanh\",\n",
    "                 dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        RNN-based language model.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self._num_layers = num_hidden_layers\n",
    "        self._hidden_layer_size = hidden_layer_size\n",
    "        self._embedding_size = embedding_size\n",
    "\n",
    "        self._embeddings = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self._rnn = nn.RNN(embedding_size, \n",
    "                           hidden_layer_size,\n",
    "                           num_layers=num_hidden_layers, \n",
    "                           dropout=dropout_rate, \n",
    "                           nonlinearity=hidden_layer_activation,\n",
    "                           batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_layer_size, vocabulary_size)\n",
    "\n",
    "        if tie_weights:\n",
    "            self._embeddings.weight = self.output_layer.weight\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize hidden states.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): batch size\n",
    "        \"\"\"\n",
    "\n",
    "        hidden = torch.zeros(self._num_layers, \n",
    "                             batch_size, \n",
    "                             self._hidden_layer_size)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, input_ids, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            input_ids (tensor): 3D tensors with batched data, or 2D tensor\n",
    "            hidden (tensor): tensor with initial hidden states\n",
    "\n",
    "        Return:\n",
    "            out (tensor): tensor of size [batch size, seq length, vocab_size]\n",
    "        \"\"\"\n",
    "\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "        return out, hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your RNN\n",
    "embedding_size = 16\n",
    "hidden_layer_size = 16\n",
    "rnn_lm_1 = RNNLM(\n",
    "    len(shakespeare_vocab),\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    ")\n",
    "print(rnn_lm_1)\n",
    "\n",
    "# output should be:\n",
    "# RNNLM(\n",
    "#   (_embeddings): Embedding(29245, 16)\n",
    "#   (_rnn): RNN(16, 16, batch_first=True)\n",
    "#   (output_layer): Linear(in_features=16, out_features=29245, bias=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a slightly different collate function for the RNN, if we want to train \n",
    "# with teacher forcing\n",
    "def rnn_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Function to construct labeled example from given batch.\n",
    "\n",
    "    Args:\n",
    "        batch (tensor): tensor of size batch_size x sentence_length with tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # we create two lists for our training examples: inputsand corresponding \n",
    "    # targets    \n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    return torch.stack(inputs, dim=0), torch.stack(targets, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your collate_fn\n",
    "# we create a toy batch of sequence_length = 4\n",
    "toy_batch = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "print(rnn_collate_fn(toy_batch))\n",
    "\n",
    "# output should be:\n",
    "# (tensor([[1, 2, 3],\n",
    "#          [5, 6, 7]]), \n",
    "#  tensor([[2, 3, 4],\n",
    "#          [6, 7, 8]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataloader with this new collate function\n",
    "rnn_training_dataloader = DataLoader(training_dataset, \n",
    "                                 collate_fn=rnn_collate_fn,\n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=True, \n",
    "                                 num_workers=0)\n",
    "print(rnn_training_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your RNN\n",
    "train(rnn_lm_1, rnn_training_dataloader, num_epochs=1, rnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate RNN\n",
    "# we use the same dataloader as with our model model, so performance is \n",
    "# comparable, but the evaluate function must be able to handle both models\n",
    "evaluate(rnn_lm_1, validation_dataloader, rnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct reverse dict to generate text with RNN\n",
    "reverse_shapeskeare_vocab = {}\n",
    "for k, v in shakespeare_vocab.items():\n",
    "    reverse_shapeskeare_vocab[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text with RNN\n",
    "def generate_text_with_rnn(num_tokens, \n",
    "                           context, \n",
    "                           model, \n",
    "                           temperature, \n",
    "                           decoding_dict):\n",
    "    \"\"\"\n",
    "    Generate num_tokens given context and model.\n",
    "\n",
    "    Args:\n",
    "        num_tokens (int): number of tokens to be generated\n",
    "        context (tensor): sequence of n token IDs in tensor of size [1, n]\n",
    "        model (RNNLM): RNN model\n",
    "        temperature (float): softmax temperature\n",
    "        decoding_dict (dict): dict of the form {token_id: token}\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "    context = torch.unsqueeze(context, 0)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    # decode predictions\n",
    "    output_tokens = []\n",
    "    for pred in predictions:\n",
    "        output_tokens.append(decoding_dict[pred])\n",
    "\n",
    "    return \" \".join(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test text generation\n",
    "\n",
    "# construct context\n",
    "context_ids = [10, 11]\n",
    "context = torch.tensor(context_ids)\n",
    "# print context\n",
    "print(\"PROMPT:\")\n",
    "for id in context_ids:\n",
    "    print(reverse_shapeskeare_vocab[id])\n",
    "print()\n",
    "\n",
    "# generate!\n",
    "# play around with temperature, higher values make distribution more uniform\n",
    "# lower values puts more mass on already probably events\n",
    "num_tokens = 10\n",
    "temperature = 1.0\n",
    "print(\"GENERATED TEXT:\")\n",
    "generate_text_with_rnn(num_tokens, \n",
    "                       context, \n",
    "                       rnn_lm_1, \n",
    "                       temperature, \n",
    "                       reverse_shapeskeare_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a modified version of the generated_text_with_rnn function. It now\n",
    "# takes as input a sample function and a value for k.\n",
    "def generate_text_with_rnn(num_tokens, \n",
    "                           context, \n",
    "                           model, \n",
    "                           temperature, \n",
    "                           decoding_dict,\n",
    "                           sampling_fn,\n",
    "                           k):\n",
    "    \"\"\"\n",
    "    Generate num_tokens given context and model.\n",
    "\n",
    "    Args:\n",
    "        num_tokens (int): number of tokens to be generated\n",
    "        context (tensor): sequence of n token IDs in tensor of size [1, n]\n",
    "        model (RNNLM): RNN model\n",
    "        temperature (float): softmax temperature\n",
    "        decoding_dict (dict): dict of the form {token_id: token}\n",
    "        sampling_fn (callable): function to produce single sample given \n",
    "                                distribution\n",
    "        k (int): number of top-k elements in distribution to sample from\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "    context = torch.unsqueeze(context, 0)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "    # decode predictions\n",
    "    output_tokens = []\n",
    "    for pred in predictions:\n",
    "        output_tokens.append(decoding_dict[pred])\n",
    "\n",
    "    return \" \".join(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-k sampling (gives us greedy with k = 1 and random with k = |V|)\n",
    "def topk_sampling(logits, k, temperature):\n",
    "    \"\"\"\n",
    "    Top-k sampling, we get greedy sampling with k = 1 and random sampling with \n",
    "    k = |V|.\n",
    "\n",
    "    Args:\n",
    "        logits (tensor): tensor of unnormalized probabilities\n",
    "        k (int): number of top-k elements in distribution to sample from\n",
    "        temperature (float): softmax temperature\n",
    "    \"\"\"\n",
    "\n",
    "    ### WRITE YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test text generation now with different sampling approaches\n",
    "\n",
    "# construct context\n",
    "context_ids = [10, 11]\n",
    "context = torch.tensor(context_ids)\n",
    "# print context\n",
    "print(\"PROMPT:\")\n",
    "for id in context_ids:\n",
    "    print(reverse_shapeskeare_vocab[id])\n",
    "print()\n",
    "\n",
    "# generate!\n",
    "# in addition to temperature, play around with different values of k\n",
    "num_tokens = 10\n",
    "temperature = 1.0\n",
    "k = 10\n",
    "print(\"GENERATED TEXT:\")\n",
    "generate_text_with_rnn(num_tokens, \n",
    "                       context, \n",
    "                       rnn_lm_1, \n",
    "                       temperature, \n",
    "                       reverse_shapeskeare_vocab,\n",
    "                       topk_sampling,\n",
    "                       k\n",
    "                       )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
